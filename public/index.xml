<?xml-stylesheet href="/rss.xsl" type="text/xsl"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>SeHe's Blog</title><link>https://sehheiden.github.io/</link><description>Recent content on SeHe's Blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 11 Feb 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://sehheiden.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>Speed Comparison on some Modern GIS tools</title><link>https://sehheiden.github.io/posts/speed_comparision_gis_intersection/</link><pubDate>Sat, 19 Oct 2024 00:00:00 +0000</pubDate><guid>https://sehheiden.github.io/posts/speed_comparision_gis_intersection/</guid><description>SeHe's Blog https://sehheiden.github.io/posts/speed_comparision_gis_intersection/ -&lt;p>GIS can change with current technology such as Apache Arrow, and perhaps also with some other techniques.
In Pythonland, Geopandas has improved performance over time. I also wanted to try Dask, DuckDB and Apache Sedona.&lt;/p>
&lt;p>Please contact me if you read this and think I could have improved the code quality and speed.
I use some public data, which is still widely used in Germany as shape files.&lt;/p>
&lt;p>&lt;strong>Update&lt;/strong>: In the previous version I was not able to save to geoparquet with DuckDB. It works, it can be openmed inm QGIS, not so far no CRS.&lt;/p>
&lt;h1 id="the-data">The data&lt;/h1>
&lt;p>First I downloaded the ALKIS (register) building data for &lt;a href="https://data.geobasis-bb.de/geobasis/daten/alkis/Vektordaten/shape/">all counties in the state of Brandenburg&lt;/a>.
All the vector files are open data. The vector files are still offered as shapefiles.
From the ALKIS dataset of Brandenburg I used the buildings and the parcels (with land use).
The files are stored per county!
The geometries have some errors, which Geopandas automatically detects and fixes.
In addition, some files cannot be opened with the &lt;a href="https://fiona.readthedocs.io/en/latest/index.html">fiona&lt;/a> library of Geopandas, with the error message
of multiple geometry columns. So we always use the new default: pyogrio.&lt;/p>
&lt;figure>&lt;img src="https://sehheiden.github.io/sanssouci_park.png"
alt="SansSouci park and palaces in Potsdam, the capital of Brandenburg">
&lt;/figure>
&lt;h1 id="task">Task&lt;/h1>
&lt;ol>
&lt;li>Open the datasets and concatenate the counties.&lt;/li>
&lt;li>Create an intersection overlay&lt;/li>
&lt;li>Save the data, if possible as a geoparquet file.&lt;/li>
&lt;/ol>
&lt;p>Why did I choose this task?
I think the overlay is one of the more computationally intensive tasks in GIS. I may write articles about other tasks later.
In &lt;a href="https://github.com/geopandas/geopandas/blob/main/geopandas/tools/overlay.py">Geopandas&lt;/a>
uses a spatial index, then calculates an intersection and joins the original data to the intersection.&lt;/p>
&lt;p>We save it as a geoparquet file, because that&amp;rsquo;s the only format Dask-GeoPandas can write to.
In addition, the result (with good compression) is small (391 MB) compared to what Geopackage (1.57 GB) needs.&lt;/p>
&lt;p>By the way, I choose which columns to open in Geopandas, because I will find out later that one column contains only &lt;code>none&lt;/code>.
So I just don&amp;rsquo;t use unimportant columns from the beginning.&lt;/p>
&lt;h1 id="the-hardware">The hardware&lt;/h1>
&lt;p>I ran the speed test on a WIN10 PV with a Ryzen 7 5800X with 48 GB of RAM.
The final runs are done with Hyperfine and 3 warm-up runs and the default 10 runs, 5 for DuckDB&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>The memory usage tests are done on a laptop with Ryzen 7 4800 and 32 GB of RAM running TuxedoOS.
The reason for this is that RAM usage is only fully recorded under Linux.&lt;/p>
&lt;h1 id="the-frameworks">The Frameworks&lt;/h1>
&lt;h2 id="geopandashttpsgeopandasorgenstableindexhtml">&lt;a href="https://geopandas.org/en/stable/index.html">Geopandas&lt;/a>&lt;/h2>
&lt;p>For me, Geopandas has been the goto solution for years.
Sometimes with some extra code, some extra libs like pyogrio.&lt;/p>
&lt;p>&lt;em>Expectations:&lt;/em> Well, nothing special. It just works. Should load faster with &lt;a href="https://pyogrio.readthedocs.io/en/latest/">pyogrio&lt;/a>.&lt;/p>
&lt;p>&lt;em>Observations:&lt;/em> Initially, loading the data takes about 75 to 80 seconds on my machine with an AMD Ryzen 5800X CPU.
It&amp;rsquo;s a bit faster when using arrow by about 15 seconds.
It got a bit slower when dropping the duplicates (on the district borders) by there &lt;code>oid&lt;/code>.&lt;/p>
&lt;p>In the end I also tried to load and build the intersection per county and then just concat the results.
It&amp;rsquo;s not faster because of the spatial indexing&amp;hellip; The RAM usage is much lower at about 3 GB.&lt;/p>
&lt;p>With the reduced number of columns, the running times are:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: left">Task&lt;/th>
&lt;th style="text-align: right">Geopandas \s&lt;/th>
&lt;th style="text-align: right">Geopandas &amp;amp; arrow \s&lt;/th>
&lt;th style="text-align: right">Geopandas &amp;amp; pyogrio, per county \s&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: left">Loading form&lt;/td>
&lt;td style="text-align: right">74&lt;/td>
&lt;td style="text-align: right">59&lt;/td>
&lt;td style="text-align: right">&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Intersection&lt;/td>
&lt;td style="text-align: right">204&lt;/td>
&lt;td style="text-align: right">181&lt;/td>
&lt;td style="text-align: right">&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Parquet&lt;/td>
&lt;td style="text-align: right">11&lt;/td>
&lt;td style="text-align: right">11&lt;/td>
&lt;td style="text-align: right">12&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Total&lt;/td>
&lt;td style="text-align: right">290&lt;/td>
&lt;td style="text-align: right">251&lt;/td>
&lt;td style="text-align: right">264&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>We have saved 3,620,994 polygons.&lt;/p>
&lt;h2 id="dask-geopandashttpsdask-geopandasreadthedocsioenstable">&lt;a href="https://dask-geopandas.readthedocs.io/en/stable/">Dask-Geopandas&lt;/a>&lt;/h2>
&lt;p>&lt;em>Expectations:&lt;/em> Partitioning the DataFrame should increase the number of cores used. This should reduce the computation time.&lt;/p>
&lt;p>&lt;em>Observations:&lt;/em> I open the shapefiles as before with Geopandas, but then convert them to a Dask-Geopandas GeoDataFrame.
All this increases the loading time a bit from about 60s to 76s. It&amp;rsquo;s not much because I don&amp;rsquo;t do the spartial partioning!&lt;/p>
&lt;p>Finally, I try the map_partitions method. On the left a Dask GeoDataFrame (the larger parcels dataset) and on the right the smaller GeoDataFrame on the right. Having the larger dataset as the Dask-GeoDataFrame increases speed.
No, spatial swapping is not necessary as the spatial index is already used.
For the map_partitions I create a function that wraps the overlay. This creates a single duplicate.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: left">Task&lt;/th>
&lt;th style="text-align: right">Geopandas \s&lt;/th>
&lt;th style="text-align: right">Dask-Geopandas \s&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: left">Loading form&lt;/td>
&lt;td style="text-align: right">59&lt;/td>
&lt;td style="text-align: right">76&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Intersection&lt;/td>
&lt;td style="text-align: right">181&lt;/td>
&lt;td style="text-align: right">62&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Parquet&lt;/td>
&lt;td style="text-align: right">11&lt;/td>
&lt;td style="text-align: right">12&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Total&lt;/td>
&lt;td style="text-align: right">251&lt;/td>
&lt;td style="text-align: right">151&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>This really does use all the cores, and you can see a usage between 30% and 95% while the Overlay
is being processed. This reduces the computing time to 33% on this machine.&lt;/p>
&lt;p>But three times faster, for 8 cores and 16 threads on the machine. Not quite what I expected.&lt;/p>
&lt;h2 id="duckdbhttpsduckdborgdocsextensionsspatialoverview">&lt;a href="https://duckdb.org/docs/extensions/spatial/overview">DuckDB&lt;/a>&lt;/h2>
&lt;p>DuckDB has a spatial extension. Although the csv/parquet file readers work well, the
tokens to load multiple files at once.
But this is not possible with ST_Read for reading spatial data. So I use pathlib as with the other frameworks.
There is no overlay, we have to do all the steps myself. So there is a possibility that my solution is suboptimal.&lt;/p>
&lt;p>I was unable to save the data geopackage due to an error in sqlite3_exec, which was unable to open the save tree.&lt;/p>
&lt;p>&lt;em>Expectation&lt;/em>: Not much, it&amp;rsquo;s stated to be faster than SQLite for DataAnalysis. Why could test before.
But how does it compare to DataFrames, which are also in RAM? Should be faster,
due to multicore usage. The memory layout benefits cannot be much, as GeoPandas also uses Apache Arrow?&lt;/p>
&lt;p>&lt;em>Observation:&lt;/em> CPU usage is high at first, but drops steadily.
For Dask the usage fluctuates. I suspect this is due to index usage. The ST_Intersects operation uses the index, ST_Intersection does not.&lt;/p>
&lt;p>The execution speed is much slower than for Dask. Saving takes so long that it is even as slow as normal geopandas.
Using the database in persistent mode (giving the connection a filename) increases the execution time.
Loading takes 70% longer, but we eliminate the need to save the data. Yes, I could load the data into a DataFrame and save it, but then it is no longer a full GeoPandas DuckDB comparison.
The saved database is actually a bit smaller than the FlatGeoBuf file.
The comparison between DuckDB and Geopandas (with arrow) in speed is&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: left">Task&lt;/th>
&lt;th style="text-align: right">Geopandas \s&lt;/th>
&lt;th style="text-align: right">DuckDB (Memory) \s&lt;/th>
&lt;th style="text-align: right">DuckDB (db-file) \s&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: left">Loading Shape&lt;/td>
&lt;td style="text-align: right">59&lt;/td>
&lt;td style="text-align: right">67&lt;/td>
&lt;td style="text-align: right">120&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Intersection&lt;/td>
&lt;td style="text-align: right">181&lt;/td>
&lt;td style="text-align: right">100&lt;/td>
&lt;td style="text-align: right">92&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Saving&lt;/td>
&lt;td style="text-align: right">11&lt;/td>
&lt;td style="text-align: right">7&lt;/td>
&lt;td style="text-align: right">&amp;mdash;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Overall&lt;/td>
&lt;td style="text-align: right">251&lt;/td>
&lt;td style="text-align: right">174&lt;/td>
&lt;td style="text-align: right">212&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Polygon Count&lt;/td>
&lt;td style="text-align: right">3620994&lt;/td>
&lt;td style="text-align: right">3619038&lt;/td>
&lt;td style="text-align: right">&amp;mdash;&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>DuckDB has a lower count in returned Polyons, but I assume that the missing are in included in the collections. (Not tested.)&lt;/p>
&lt;h2 id="apache-sedonahttpssedonaapacheorglatest-with-pyspark">&lt;a href="https://sedona.apache.org/latest/">Apache Sedona&lt;/a> with PySpark&lt;/h2>
&lt;p>&lt;em>Expectations:&lt;/em> Some loss due to virtualization with Docker.
So PySpark would not be as fast as Dask?&lt;/p>
&lt;p>Although the code is conceptually very similar to the database version. It is an interesting technology.
I started with the Sedona container as a docker-compose file. This created a local Spark instance with Sedona and Jupyter notebook.&lt;/p>
&lt;p>The shapefiles can be loaded with a placeholder. No iteration (in code) required.
But we need to validate the geometry with ST_MakeValid. Otherwise, I get a Java error message which is really long.
Which makes it hard to understand, at least if you are not used to it.
You can use SQL syntax on the DataFrames, or you can use message chaining methods.
I started with SQL code (which is more universal), but it contains long strings in your code.
Once everything was working, I switched to method chaining. Which in my eyes looks better, more functional.
This flexibility is a plus.&lt;/p>
&lt;p>So far the code is lazy. A show method only executes on the row it will show, counting on all rows.
Lazy execution can lead to double execution, so I remove all count methods.
The slowest part seems to be writing. But differentiating the timing is difficult due to the lazy execution.
The data is saved as a geoqarquet file with 3618339 polygons, the size was about 320 MB with snappy compression and 250 MB with ZSTD.
Saving as a single parquet file takes about 158 seconds.
I would have liked to use more containers on a single device and let them talk to each other to get multiple workers and a master to see how much multi-node
reduced the computation time further.&lt;/p>
&lt;p>But that did not seem so trivial (please prove me wrong)&lt;/p>
&lt;h1 id="overall-speed-comparison">Overall speed comparison&lt;/h1>
&lt;p>For the overall speed and memory usage comparison, I exclude Apache-Sedona as it is running in a Docker container (for now).&lt;/p>
&lt;p>The previous timings were based on warmed runs, but single executions. Here we use 10 warmed runs to get a better picture.
We need the warming because we use the same input data, so a GeoPandas run would also warm up the Dask GeoPandas run, and so on.
Without warming GeoPandas, this would be even slower.
These execution times must always be slower than the previous ones, because they include loading the Python interpreter and all libraries. and all libraries.&lt;/p>
&lt;p>Without using arrow for input data. GeoPandas took 287.519 s ± 1.532 s to open, overlay and save.
The overall variation will be small.&lt;/p>
&lt;p>Opening the files with the &lt;code>use_arrow&lt;/code> option reduces the computation time by about 8%. The execution takes: 264.590 s ± 4.891 s.&lt;/p>
&lt;p>With dask, the speed decreases slightly. The addition of multicore reduces the total execution time by a third. On the other hand,
I still have a good part (loading the data) that is limited to single core. So we end up with an execution time of 169.577 s ± 1.882 s.&lt;/p>
&lt;p>For DuckDB with in-memory (and saving to FlatGeoBuf) the execution time is 271.455 s ± 2.790 s, and when persisting and not writing the final result it takes 233.427 s ± 6.805 s.&lt;/p>
&lt;h1 id="total-memory-usage-comparison">Total Memory Usage Comparison&lt;/h1>
&lt;p>The question here is whether it is necessary to hold all the data in RAM at the same time, or whether a good strategy can reduce RAM usage.
To hold only the final result and partitions of the input data.&lt;/p>
&lt;p>The input data accounts for about 8 GB of RAM usage. This is the plateau in RAM usage of the Geopandas program. RAM usage peaks at just under 14 GB.&lt;/p>
&lt;figure>&lt;img src="https://sehheiden.github.io/geopandas_ram.png"
alt="RAM usage of GeoPandas">
&lt;/figure>
&lt;p>Dask seems to need several copies of the input data. We have a first plateau at about 8 GB and a
at about 12 GB. The RAM usage peaks at about 19 GB.&lt;/p>
&lt;figure>&lt;img src="https://sehheiden.github.io/dask_geopandas_ram.png"
alt="RAM usage of Dask-GeoDataFrame">
&lt;/figure>
&lt;p>The memory layout in DUCKDB greatly reduces the peak memory usage. I can also use a view
for the final result, which adds up to even more savings.
Frankly, at only about 7 GB, the peak RAM usage is smaller than the input data in GeoPandas.
The input data alone uses about 3.5 GB.&lt;/p>
&lt;figure>&lt;img src="https://sehheiden.github.io/duckdb_memory_ram.png"
alt="RAM usage of DuckDB (in-memory)">
&lt;/figure>
&lt;p>The persistence of the database does not change much, the input data seems to be even smaller at 2.5 GB.
The top RAM usage is also reduced to about 6.2 GB.
&lt;figure>&lt;img src="https://sehheiden.github.io/duckdb_persisted_ram.png"
alt="RAM usage of DuckDB (persistent)">
&lt;/figure>
&lt;/p>
&lt;h1 id="conclusion">Conclusion&lt;/h1>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: left">Package&lt;/th>
&lt;th style="text-align: right">Total duration \s&lt;/th>
&lt;th style="text-align: right">Top RAM usage \GB&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: left">Geopandas&lt;/td>
&lt;td style="text-align: right">287.5 ± 1.5&lt;/td>
&lt;td style="text-align: right">&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Geopandas (arrow)&lt;/td>
&lt;td style="text-align: right">264.6 ± 4.9&lt;/td>
&lt;td style="text-align: right">14&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Dask-Geopandas&lt;/td>
&lt;td style="text-align: right">169.6 ± 1.9&lt;/td>
&lt;td style="text-align: right">19&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">DuckDB (in memory)&lt;/td>
&lt;td style="text-align: right">167.2 ± 3.0&lt;/td>
&lt;td style="text-align: right">7&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">DuckDB (persistent)&lt;/td>
&lt;td style="text-align: right">233.4 ± 6.8&lt;/td>
&lt;td style="text-align: right">6.2&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>The intersection itself has a speedup S of about three for Dask-GeoDataFrames and two for DuckDB compared to GeoPandas.
This is despite using 8 cores with hyper-threading.
I suspect that DuckDB is slower here because intersection does not use the spatial index, but intersection does.
But DuckDB makes it up with faster loading and saving.
When we are able to use multiple cores, loading the data becomes a relatively long part of the total execution time.
Either a distribution in geoparquet or loading each file in a separate thread could help.&lt;/p>
&lt;p>For Apache-Sedona we can only compare the total execution time and this seems to be on par with Dask-GeoPandas.&lt;/p>
&lt;p>If low memory usage is important, DUCKDB is a good option. So either on systems with low memory, or with huge amounts of data.
To avoid using swap. Opening shapefiles with DuckDB is slower than with GeoPandas, but that is acceptable, as the exceution is faster.
So far The number of supported file formats can be a deal breaker. But I enjoy geoparquet sofar.
Remember, DuckDB does not support raster files.&lt;/p>
&lt;p>If you already have a Spark cluster, Sedona may be a valid option. So far Dask is the fastest solution, but uses a huge amount of additional memory.
Maybe one day I can recommend DuckDB instead.&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>While using hyperfine on the DuckDB code, I found that a file is created in the temp folder for each output.
These files have an uuid part and are never deleted, eg: &lt;code>buildings_with_parcels_1976_2_temp.fgb&lt;/code>.
This seems to be a real bug in DuckDB.
So while profiling with hyperfine, the temp files are written to disk until the main drive is full.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div>
- https://sehheiden.github.io/posts/speed_comparision_gis_intersection/ -</description></item><item><title>About</title><link>https://sehheiden.github.io/about/</link><pubDate>Sat, 28 Sep 2024 22:04:08 +0200</pubDate><guid>https://sehheiden.github.io/about/</guid><description>SeHe's Blog https://sehheiden.github.io/about/ -&lt;h2 id="about-me">About Me&lt;/h2>
&lt;p>Hello! I&amp;rsquo;m Sebastian, and this is my blog.&lt;/p>
&lt;figure>&lt;img src="https://sehheiden.github.io/profile.png"
alt="Profile Picture" width="100px">
&lt;/figure>
- https://sehheiden.github.io/about/ -</description></item><item><title>LDA and WordCloud on Mastodon Posts for the Bavarian State Election!</title><link>https://sehheiden.github.io/posts/mastodonbavarian_election_lda/</link><pubDate>Sat, 04 May 2024 00:00:00 +0000</pubDate><guid>https://sehheiden.github.io/posts/mastodonbavarian_election_lda/</guid><description>SeHe's Blog https://sehheiden.github.io/posts/mastodonbavarian_election_lda/ -&lt;p>We selected a party as the dominant party, when this party or its
candidate was mentioned 50 % or more compared to other parties. But this
results in many posts that mention the candidates Söder, Aiwanger,
although the post is attribute to different parties. This mixed
attribution, which allows to mention different parties as long as there
is a clear dominant party, makes attribution hard, but was used to keep
as many posts as possible. Even though in some weeks we did not record
post for some smaller &amp;quot;dominant&amp;quot; parties.&lt;/p>
&lt;p>In addition, the state election in Hessia, was held on the same day,
which also mentioned in some posts.&lt;/p>
&lt;p>We use the German StopWords from NLTK. Also use the German StopWords form Spacy.&lt;/p>
&lt;h1 id="word-cloud-after-lemmatize">Word Cloud after Lemmatize&lt;/h1>
&lt;p>Lemmatize with Spacy (&lt;a href="https://spacy.io/models/de">https://spacy.io/models/de&lt;/a>). Spacy uses models
small to large and a BERT based model. We use &lt;em>large&lt;/em>. NLTK WORDNET does not work with the German
language.&lt;/p>
&lt;p>The Wordcloud shows, that the candidate Hubert Aiwanger and Markus Söder
were most important, as their parties Freie Wähler, CDU/CSU (Union) and
the party AFD.&lt;/p>
&lt;p>In addition, the words Nazi and Antisemitismus were frequently used.
Also, the name of the state &lt;code>Bayern&lt;/code> was very frequent. But we had a
regional filter criteria, so that the post had to name the state name,
or that of any other local entity at any local government level or
candidate name.&lt;/p>
&lt;h1 id="wordclouds-for-each-party">WordClouds for each Party&lt;/h1>
&lt;p>The word cloud of each party can be selected with the drop box.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>party&lt;/th>
&lt;th style="text-align: right">support&lt;/th>
&lt;th>noteworthy frequent terms&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>AFD&lt;/td>
&lt;td style="text-align: right">609&lt;/td>
&lt;td>&lt;code>rechtsextrem&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>CSU&lt;/td>
&lt;td style="text-align: right">1570&lt;/td>
&lt;td>&lt;code>Söder&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>FDP&lt;/td>
&lt;td style="text-align: right">94&lt;/td>
&lt;td>&lt;code>Ampel&lt;/code>, &lt;code>fliegen&lt;/code> and &lt;code>grün&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Freie Wähler&lt;/td>
&lt;td style="text-align: right">2015&lt;/td>
&lt;td>&lt;code>Hubert Aiwanger&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Bündnis 90/Die Grünen&lt;/td>
&lt;td style="text-align: right">54&lt;/td>
&lt;td>&lt;code>Hartmann&lt;/code>, &lt;code>Grün&lt;/code>, &lt;code>Bayern&lt;/code>, &lt;code>Frau&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SPD&lt;/td>
&lt;td style="text-align: right">192&lt;/td>
&lt;td>&lt;code>Bayern&lt;/code>, &lt;code>Hessen&lt;/code>, &lt;code>Israel&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Die Linke&lt;/td>
&lt;td style="text-align: right">69&lt;/td>
&lt;td>&lt;code>dropbox&lt;/code>, &lt;code>ordner&lt;/code> and &lt;code>icloud drive&lt;/code>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;hr>
&lt;p>CSU and Freie Wähler posts mention their candidates very often. These
posts attributed to other parties are more diverse. The posts attributed
to &lt;code>Die Linke&lt;/code> mention exchange directories. FDP posts seam to be more
about the national government (&lt;code>Ampel&lt;/code>). The few posts about the part
&lt;code>Die Grüne&lt;/code> are the only ones to mention the term Frau (woman) often!&lt;/p>
&lt;p>Filtering out by computing the mean(for groups of topic, week and
party).&lt;/p>
&lt;p>Get the most important words of a given model.&lt;/p>
&lt;p>There are to many topics with the same most important word therefore we use two.
We show them in Alphabetical order.&lt;/p>
&lt;pre tabindex="0">&lt;code> {0: &amp;#39;aiwanger bayern&amp;#39;,
1: &amp;#39;afd aiwanger&amp;#39;,
2: &amp;#39;afd aiwanger&amp;#39;,
3: &amp;#39;aiwanger söder&amp;#39;,
4: &amp;#39;aiwanger csu&amp;#39;,
5: &amp;#39;aiwanger csu&amp;#39;,
6: &amp;#39;afd bayern&amp;#39;,
7: &amp;#39;aiwanger csu&amp;#39;,
8: &amp;#39;aiwanger csu&amp;#39;,
9: &amp;#39;aiwanger söder&amp;#39;}
&lt;/code>&lt;/pre>&lt;p>value_counts:&lt;/p>
&lt;pre tabindex="0">&lt;code> important_words
aiwanger csu 1966
aiwanger söder 954
afd aiwanger 657
aiwanger bayern 548
afd bayern 478
&lt;/code>&lt;/pre>&lt;h1 id="lda">LDA&lt;/h1>
&lt;p>Seeing the phi_k correlation measure, there is a medium correlation
between dominant party and topic. Which makes sense, the party names and
their candidate names are contained in the most important terms for some
topics! The sentiment seam to be weakly or not correlated.&lt;/p>
&lt;p>The word cloud of the original data, shows how import it is to remove
stop words, as the most frequent terms as connotations and article do
not have any meaning.&lt;/p>
&lt;h1 id="lda---topics">LDA - Topics&lt;/h1>
&lt;p>The actual topic index, may change a possible, tested result is for 15
topics: Some Topics form cluster. The main cluster consists of the
topics 1-6, 8, 10, 12 (Bavaria, CSU, Söder, AFD). A smaller cluster is 7
&amp;amp; 9 (CSU).&lt;/p>
&lt;p>The weighting factor λ is applied to rank the terms. λ = 1, means
ranking of terms in decreasing order of their &lt;em>topic&lt;/em>-specific
probability and λ = 0 ranking by lift (term probability within a topic
over its marginal probability across the corpus). Meaning, λ = 1 favours
total probability with the corpus and λ = 0 favours the probability with
the current text (see
&lt;a href="https://nlp.stanford.edu/events/illvi2014/papers/sievert-illvi2014.pdf">https://nlp.stanford.edu/events/illvi2014/papers/sievert-illvi2014.pdf&lt;/a>).&lt;/p>
&lt;p>The clusters 11 (Freie Wähler, CSU), 13 (Söder, Bayern, AFD), 14 (Söder,
Bayern) and 15 (Bayern) are isolated. The most import terms are:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: right">Topic&lt;/th>
&lt;th>Percentage&lt;/th>
&lt;th>λ = 0&lt;/th>
&lt;th>λ = 0.5&lt;/th>
&lt;th>λ = 1&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: right">1&lt;/td>
&lt;td>9.8&lt;/td>
&lt;td>Nazi, Augsburg&lt;/td>
&lt;td>CSU&lt;/td>
&lt;td>CSU&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: right">2&lt;/td>
&lt;td>8.9&lt;/td>
&lt;td>1, 25&lt;/td>
&lt;td>Söder, CSU&lt;/td>
&lt;td>Söder, CSU&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: right">3&lt;/td>
&lt;td>7.8&lt;/td>
&lt;td>Prozent&lt;/td>
&lt;td>AFD, Bayern&lt;/td>
&lt;td>AFD, Bayern&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: right">4&lt;/td>
&lt;td>7.7&lt;/td>
&lt;td>Hessen, Wählen (elect)&lt;/td>
&lt;td>Söder, Bayern&lt;/td>
&lt;td>Söder, Bayern&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: right">6&lt;/td>
&lt;td>7.1&lt;/td>
&lt;td>ltbwby23&lt;/td>
&lt;td>CSU, Bayern&lt;/td>
&lt;td>CSU, Bayern&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: right">7&lt;/td>
&lt;td>7.0&lt;/td>
&lt;td>werfen (through)&lt;/td>
&lt;td>CSU, Söder&lt;/td>
&lt;td>CSU, Söder&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: right">9&lt;/td>
&lt;td>6.4&lt;/td>
&lt;td>DE, Fakt (fact)&lt;/td>
&lt;td>CSU&lt;/td>
&lt;td>CSU, CDU&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: right">10&lt;/td>
&lt;td>5.8&lt;/td>
&lt;td>Welt (world), Opfer (victim)&lt;/td>
&lt;td>Hubert, Söder&lt;/td>
&lt;td>Hubert, Söder&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: right">11&lt;/td>
&lt;td>5.5&lt;/td>
&lt;td>Freie Wähler&lt;/td>
&lt;td>Freie Wähler, Hubert, CSU, Söder&lt;/td>
&lt;td>Freie Wähler,CSU, Hubert, Söder&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: right">12&lt;/td>
&lt;td>5.2&lt;/td>
&lt;td>Hartmann&lt;/td>
&lt;td>CSU, Bayern, AFD&lt;/td>
&lt;td>CSU, Bayern, AFD&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: right">13&lt;/td>
&lt;td>5.0&lt;/td>
&lt;td>wissen (know, knowledge)&lt;/td>
&lt;td>Söder, Bayern,&lt;/td>
&lt;td>Söder, Bayern&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: right">14&lt;/td>
&lt;td>4.9&lt;/td>
&lt;td>Hubsi (nickname of Hubert Aiwanger), Augsburg&lt;/td>
&lt;td>Söder, Augsburg, Söder, Bayern&lt;/td>
&lt;td>Bayern&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: right">15&lt;/td>
&lt;td>4.7&lt;/td>
&lt;td>2, FDP&lt;/td>
&lt;td>Bayern&lt;/td>
&lt;td>Bayern&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>I removed the tpoics with to many/long terms.&lt;/p>
&lt;h1 id="lda-for-each-party-with-up-to-15-topics">LDA for each party (with up to 15 topics)&lt;/h1>
&lt;p>When several topics are attributed to a document. How strong the
attribution is given by a ratio. The sum of these ratios per document
is 1. We attribute each document with its strongest associated topic.
Therefore, some topics are not included in the main_topics series.&lt;/p>
&lt;p>Computation of the LDA for the party &lt;code>Die Linke&lt;/code> does not work. Many
(most) topics share the same most important term.&lt;/p>
&lt;p>Because my topics share the same most important word, we use the two
most import words (in alphabetical order). This increases the number of
topics with unique most import words, while the most import words stay
distinguishable.&lt;/p>
&lt;p>Interesting most important words are:&lt;/p>
&lt;ul>
&lt;li>CSU: (braun, Söder), (frage, mehrere)&lt;/li>
&lt;li>FDP: (dienen, Söder), (Bundesregierung, dienen)&lt;/li>
&lt;li>Freie Wähler: (koalition, nazi)&lt;/li>
&lt;li>SPD: (dienen, Söder), (Aiwanger, Kampagne), (Aiwanger, jung)&lt;/li>
&lt;/ul>
&lt;p>As these terms are a good summary of the most important discussions. As
minister Aiwanger did have a Nazi pamphlet in his school bag when he was
young. Prime minister Söder did have several questions, about that, that
ha to be answered.&lt;/p>
&lt;h1 id="conclusion">Conclusion&lt;/h1>
&lt;p>Contrary to our hopes, there is no correlation between the topics and
the sentiment. This might be caused, by the fact that topics are very
similar, or that we attribute posts to a party by its dominant party.&lt;/p>
- https://sehheiden.github.io/posts/mastodonbavarian_election_lda/ -</description></item><item><title>Mastodon - Election Predictions</title><link>https://sehheiden.github.io/posts/mastodon_on_bavarian_elecvtion/</link><pubDate>Fri, 15 Dec 2023 00:00:00 +0000</pubDate><guid>https://sehheiden.github.io/posts/mastodon_on_bavarian_elecvtion/</guid><description>SeHe's Blog https://sehheiden.github.io/posts/mastodon_on_bavarian_elecvtion/ -&lt;p>I held a presentation on this project on Elixir MeetUp Berlin on Feb 8th, 2024. The slides can be found &lt;a href="https://github.com/sehHeiden/MeetUpElixir/blob/main/presentation_MastodonElection.pdf">here&lt;/a>.&lt;/p>
&lt;p>The Livebook code can be found &lt;a href="election_bavaria.livemd">here&lt;/a>.
The code for data collection can be found &lt;a href="./mastodon">here&lt;/a>.&lt;/p>
&lt;h1 id="abstract">Abstract&lt;/h1>
&lt;p>We try to predict voting result in the 2023 Bavarian state election in Germany by Mastodon posts.&lt;/p>
&lt;p>The last polls before the election show an average error of about 0.7 to 0.9 percent per major party. A time weighted average of the polls of the last six weeks before the election reduced the error to 0.39 percentage points per party.&lt;/p>
&lt;p>We apply frequency based and sentiment based methods on the posts:&lt;/p>
&lt;ul>
&lt;li>frequency based
&lt;ul>
&lt;li>Bavarian or other German posts: between Aug. 29th and Oct. 7th.&lt;/li>
&lt;li>Bavarian or other German posts: between Sep. 17th and Oct. 7th.&lt;/li>
&lt;li>Most positive post per Bavarian or other German author: between Sep. 17th and Oct. 7th.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>sentiment based
&lt;ul>
&lt;li>weighted weekly sentiment.&lt;/li>
&lt;li>fit of weekly averaged polls versus weekly averaged sentiments.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>We have a total of 160 Bavarian users with evaluable posts. The sample size is to low to ensure high significance to represent the population.
Applying frequency based methods on the Mastodon posts shows a best error of 4.4 percentage points per party. If we fit the polls versus the sentiment, we get an error 0.40 percentage points per party compared to the election result.
While the average of the polls in the last six weeks have an error 0.39. The dependency on the sentiment is only about 2.5 percentage points the rest can be attributed to the parties themselves.&lt;/p>
&lt;h1 id="introduction">Introduction&lt;/h1>
&lt;p>Mastodon&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup> is a micro blogging service that is federated by the ActivityPub protocol and part of the fediverse.&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup> Depending on the source Mastodon has about 8.4 million users world wide,&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup> or 14.4 M users both on Oct. 10th 2023.&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>
We started the monitoring on Aug. 29th 2023. The end date of data generation was Nov. 19th 2023. The Aiwanger affaire took place at the beginning of the period. The daily news paper the &lt;code>SZ&lt;/code> published an article that states that the Bavarian vice minister president and candidate of the &lt;code>FW&lt;/code> Hubert Aiwanger&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup> was carrying an anti-Semitic pamphlet in is satchel when he was 17 years old.&lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>&lt;/p>
&lt;p>A study by the public service broadcasting stations ARD and ZDF shows that about two percent of all Germans, of both women an man are weekly recurring Mastodon users. This means about 1.7 M weekly recurring users from Germany.
Mastodon reaches up to three percent for those aged 14 to 49.&lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>
Because many services (and their instances) are able to federate with each other, it is possible to read posts from other services like Misskey, Lemmy, Pixelfed, Wordpress-Blogs.&lt;/p>
&lt;p>Mastodon instances are often centred around a topic or a region. There is a high number of regional instances.&lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup> Many are German. Mastodon had a strong spike in usage in Nov. 2022 with 2.5 M monthly recurrent users. Currently the network has still 1.6 million monthly recurrent users.&lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>
We analyse Mastodon toots with the topic Bavaria state election which took place on Oct. 8th 2023. We apply a sentiment analysis. Sentiment analysis is used in natural language processing. The aim is recognise positive, neutral, or negative attitude within the text.&lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>
We attempt to differentiate into Bavaria and other German regions.&lt;/p>
&lt;p>We use the sentiment and frequency of mentioned persons and parties (see tab. 1). We do not consider favourisation, reblogs or replies. In addition, we did not try to monitor voting intention.&lt;sup id="fnref:11">&lt;a href="#fn:11" class="footnote-ref" role="doc-noteref">11&lt;/a>&lt;/sup>
We will use the terms posts and toots interchangeably. We call a user that wrote a particular post an author. One limiting issue is the sample size of users that use the Mastodon and other federated services. The other is demographics.&lt;/p>
&lt;h2 id="monitored-parties">Monitored Parties&lt;/h2>
&lt;p>The sentiment was monitored for the candidates and parties&lt;sup id="fnref:12">&lt;a href="#fn:12" class="footnote-ref" role="doc-noteref">12&lt;/a>&lt;/sup> in tab. 1.&lt;sup id="fnref:13">&lt;a href="#fn:13" class="footnote-ref" role="doc-noteref">13&lt;/a>&lt;/sup>&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: left">Party&lt;/th>
&lt;th style="text-align: left">Candidate(s)&lt;/th>
&lt;th style="text-align: right">Percentage 2018&lt;/th>
&lt;th style="text-align: right">Percentage 2023&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: left">AfD&lt;/td>
&lt;td style="text-align: left">Katrin Ebner-Steiner &amp;amp; Martin Böhm&lt;/td>
&lt;td style="text-align: right">10.2&lt;/td>
&lt;td style="text-align: right">14.6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">CSU&lt;/td>
&lt;td style="text-align: left">Markus Söder&lt;/td>
&lt;td style="text-align: right">37.2&lt;/td>
&lt;td style="text-align: right">37.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">FDP&lt;/td>
&lt;td style="text-align: left">Martin Hagen&lt;/td>
&lt;td style="text-align: right">5.1&lt;/td>
&lt;td style="text-align: right">3.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Freie Waehler&lt;/td>
&lt;td style="text-align: left">Hubert Aiwanger&lt;/td>
&lt;td style="text-align: right">11.6&lt;/td>
&lt;td style="text-align: right">15.8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Gruene&lt;/td>
&lt;td style="text-align: left">Ludwig Hartmann &amp;amp; Katharina Schulze&lt;/td>
&lt;td style="text-align: right">17.6&lt;/td>
&lt;td style="text-align: right">14.4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">SPD&lt;/td>
&lt;td style="text-align: left">Florian von Brunn&lt;/td>
&lt;td style="text-align: right">9.7&lt;/td>
&lt;td style="text-align: right">8.4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Linke&lt;/td>
&lt;td style="text-align: left">Adelheid Rupp&lt;/td>
&lt;td style="text-align: right">3.2&lt;/td>
&lt;td style="text-align: right">1.5&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h1 id="methods">Methods&lt;/h1>
&lt;p>We have two phases in the project. The first phase is monitoring the data from Mastodon. In the second phase we try to use the posts from Mastodon to predict the voting outcome.&lt;/p>
&lt;h2 id="monitoring">Monitoring&lt;/h2>
&lt;p>Following tags are monitored on the instance &lt;em>chaos.social&lt;/em>. We group the tags by topics (see tab. 2).&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Category&lt;/th>
&lt;th>Keywords&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Bavaria&lt;/td>
&lt;td>bayern, bayernwahl, bayernwahl2023&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Election&lt;/td>
&lt;td>wahlen, wahlkampf, wahlumfrage, wahlen23, wahlen2023&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Partys&lt;/td>
&lt;td>spd, csu, gruene, grune, gruenen, grunen, afd, freiewaehler, freiewahler, fw, fpd, linke&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Candidates&lt;/td>
&lt;td>markussoeder, markussoder, soeder, soder, hubertaiwanger, aiwanger, hartmann, martinhagen, ebnersteiner&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;!-- raw HTML omitted -->
&lt;p>Some candidates were not included, because their names were not used as tags at the beginning of the study. We only used German words as tags.&lt;/p>
&lt;p>A wide set of topics have been selected to retrieve a maximum of tagged posts. Due to the concept of federation of instances, it is possible that not all instances share posts, or not all posts. Still, only a single instance has been monitored to reduce the need of removing duplicates with different ids on each instance.&lt;/p>
&lt;p>Search of posts without the need of tags has been released during the monitoring with Mastodon version 4.2 in the end of Sep. 2023. It was added on Oct. 3rd on chaos.social. Reindexing has been finished on Oct. 5th. We added search on Oct. 7th to the monitoring, the day before the election. The end date of data generation was Nov. 19th 2023.&lt;/p>
&lt;p>We retrieve the tags via the public timeline of the instance and the search via the search api:&lt;/p>
&lt;ul>
&lt;li>&lt;em>{{instance_url}}&lt;/em>/api/v1/timelines/tag/&lt;em>{{tag_name}}&lt;/em>&lt;/li>
&lt;li>&lt;em>{{isntance_url}}&lt;/em>/api/v2/search?q=&lt;em>{{search_word}}&lt;/em>&lt;/li>
&lt;/ul>
&lt;p>Search of tags in the public timeline is done without a login; therefore, only public posts are monitored. For the search a bearer token is needed. During the addition of the search the limit of requested post was increased from 20 (default) to 40 (maximum).&lt;/p>
&lt;p>The posts are requested every full hour starting Aug. 29th 2023. The retrieval is done with a Elixir program that runs on Erlang&amp;rsquo;s BEAM runtime, to increase stability. Each post is written into four tables of a SQLite3 database. The &lt;code>toots&lt;/code> table contains the post itself and some of its meta data. The &lt;code>users&lt;/code> table contains some meta data of the posts about the users, who wrote the posts. The related table &lt;code>fields&lt;/code> contains the fields a user can set using key value pairs, to add some information about him-/herself. The related &lt;code>tags&lt;/code> table contains all tags of each post.&lt;/p>
&lt;p>Depending on the search term and how often it is used in posts, the time range of a look back of 20/40 posts varies. For some search terms that are less frequently used, it lasts back several years. For other terms it ranges back several days only. This behaviour partially heals interruptions in the monitoring service.&lt;/p>
&lt;h2 id="data-analysis">Data Analysis&lt;/h2>
&lt;p>After retrieving the first batch of data, we start with a subsample, with the data of the first twelve days. While
we still fetch new data. The subsample is used to create and test a work flow for the data analysis.&lt;/p>
&lt;h3 id="data-preparation">Data Preparation&lt;/h3>
&lt;p>The evaluation is done in an &lt;a href="election_bavaria.livemd">Elixir Livebook&lt;/a>. First exploration was done on a sub-sample of the dataset, which was recorded until Sep. 10th 2023, about 12 days of full records. This dataset was used to fine tune the analysis. This notebook is than applied on the &lt;em>full-sample&lt;/em>.
The texts are cleaned to exclude:&lt;/p>
&lt;ul>
&lt;li>Html tags.&lt;/li>
&lt;li>Links.&lt;/li>
&lt;li>Characters: #, @, and _ .&lt;/li>
&lt;li>Removed double spaces.&lt;/li>
&lt;/ul>
&lt;p>The decision whether a post can be used for further analysis is done by filtering in three stages:&lt;/p>
&lt;ol>
&lt;li>Cleaned text contains more than 50 characters of cleaned text.&lt;/li>
&lt;li>Keep only posts specific to the region Bavaria.&lt;/li>
&lt;li>Keep only posts that is attributable to a single or dominant party.&lt;/li>
&lt;/ol>
&lt;h3 id="regional-filter">Regional Filter&lt;/h3>
&lt;p>The regional filter accepts any post that abides to any of the the three rules:&lt;/p>
&lt;ol>
&lt;li>(Raw) post must name of any local entity.&lt;/li>
&lt;li>(Raw) post must name of any candidate.&lt;/li>
&lt;li>(Raw) post must name &lt;code>CSU&lt;/code> (single regional party).&lt;/li>
&lt;/ol>
&lt;p>We use 3890 local entities from the state name down to village names. We do not use the 7860 sub-district names as their number is much higher and includes some major common German words (e.g. Gern, Oder). This is also partially true for villages names (e.g. Wald). This filter method may also keep posts which mention similar names in other regions of the world.&lt;/p>
&lt;h2 id="attribute-sentiment-to-party">Attribute Sentiment to Party&lt;/h2>
&lt;p>To select (raw) posts that&amp;rsquo;s sentiment can be attributed to a party, we test two methods:&lt;/p>
&lt;ul>
&lt;li>Keep posts that only name a single party or their candidate(s).&lt;/li>
&lt;li>Keep posts that contains a dominant party that is mentioned more often than all other party combined.&lt;/li>
&lt;/ul>
&lt;h3 id="spatial-differentiation">Spatial Differentiation&lt;/h3>
&lt;p>Based on OSM - Mastodon server, Mastodon instances are estimated to be used by Bavarian users (see &lt;code>List of Bavarian Instances&lt;/code> in the Appendix).&lt;sup id="fnref1:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup> In addition, the fields and notes of each user are scanned for Bavarian location names by Bavarian State Office for Survey and Geoinformation from the state name down to village name (see fig. 1).&lt;sup id="fnref:14">&lt;a href="#fn:14" class="footnote-ref" role="doc-noteref">14&lt;/a>&lt;/sup>&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;p>Personal information can be stored in the use profile, e.g. in the user fields und the user text. The fields are key value stores which is a precise method to store the data. Location names that are common German words will not be misunderstood, whereas locations with same names, but in different regions are kept. If filtering by the user text, we can not filter by the keys above, but rely on the entity names alone. The full region selection algorithm is shown below.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;h3 id="sentiment-analysis">Sentiment Analysis&lt;/h3>
&lt;p>The minimum of 50 characters was used to reduce the misclassification that may happen on shorter texts. The posts all contain a language label, but this is set by the user or his/her application and is therefore error prone. We detect the language by the roberta base language detection model with a limit 0f 100 tokens.&lt;sup id="fnref:15">&lt;a href="#fn:15" class="footnote-ref" role="doc-noteref">15&lt;/a>&lt;/sup>
The German sentiment analysis is done with the German sentiment bert model.&lt;sup id="fnref:16">&lt;a href="#fn:16" class="footnote-ref" role="doc-noteref">16&lt;/a>&lt;/sup> We use the maximum limit of 512 possible tokens of the model. We only use the first 512 tokens and do not combine the analysis of multiple sections of the text. 512 tokens is far longer than the maximum post length of Mastodon of 500 characters as default. OPENAI estimates 4 characters per token, but this figure does vary per language and tokenizer.&lt;sup id="fnref:17">&lt;a href="#fn:17" class="footnote-ref" role="doc-noteref">17&lt;/a>&lt;/sup> The German Sentiment Bert model is available as a python package. We applied the alternative to use the Elixir library Bumblebee.
The English language posts are evaluated with the bertweet model with the limit of 130 tokens (see fig. 2).&lt;sup id="fnref:18">&lt;a href="#fn:18" class="footnote-ref" role="doc-noteref">18&lt;/a>&lt;/sup>&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;p>The whole language classification process is shown below. The sentiment is mapped from the three classes (positive, negative, neutral) to a range -1 (negative) to 1 (positive).&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;h3 id="sentiment-graphs">Sentiment Graphs&lt;/h3>
&lt;p>The Sentiment is shown for the following regions:&lt;/p>
&lt;ul>
&lt;li>Bavaria.&lt;/li>
&lt;li>Other German.&lt;/li>
&lt;/ul>
&lt;p>For all parties most sentiments are neutral or negative. Irony detection is not included.&lt;sup id="fnref:19">&lt;a href="#fn:19" class="footnote-ref" role="doc-noteref">19&lt;/a>&lt;/sup>&lt;/p>
&lt;h3 id="correlate-polls-and-sentiments">Correlate Polls and Sentiments&lt;/h3>
&lt;p>The polls which have a start and end date are converted into a daily and weekly timeline for each party. For the daily poll timeline each day form start day to end day is unrolled and given the poll results for a certain party. If a day has a value estimated by different polling agencies, the poll results are averaged for each party. Missing daily values are filled with forward feed first and then with a backward feed. The days are encoded as day of the year.
For the weekly timeline we take median day when each poll was executed. This median day was converted into a calendar week. If different polls were made, the results are averaged for each party.
The daily and weekly sentiment data were converted and filled in a similar fashion.
The timelines are aligned in time by latest start date and earliest end date. The alignments in values is done by converting the poll results into fractions and mapping the range of the sentiment values of -1 to 1 to a ratio of the sentiments of all parties from 0 to 1.&lt;/p>
&lt;p>We want to estimate either the polling or sentiment time line delayed, and how long. Therefore we estimate the cross correlation between the timelines. We adjust the timelines further by the offset between them. For the adjusted timeline we estimate the linear regression between sentiment and polling.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;h1 id="results">Results&lt;/h1>
&lt;p>As a reference the sentiment analysis will be compared to polls.&lt;/p>
&lt;h2 id="polls">Polls&lt;/h2>
&lt;p>Polls from different sources are listed at wahlrecht.de.&lt;sup id="fnref:20">&lt;a href="#fn:20" class="footnote-ref" role="doc-noteref">20&lt;/a>&lt;/sup> With that we construct the timeline of a meta poll. The timeline for each party is shown below. The emphasis local changes in the fit, we apply the &lt;code>Locally estimated Scatterplot Smotting&lt;/code> (Loess) fit with a bandwidth of 0.5 on the median datetime of each poll in dark red. The result of the election is shown as a blue line.&lt;/p>
&lt;p>The strongest party &lt;code>CSU&lt;/code> loses about three percent points since the start of the year 2023 with an result of 37 % at the election. In contrast, its coalition partner &lt;code>Freie Waehler&lt;/code> (&lt;code>FW&lt;/code>) increases by five percent points and wins 15.8 % of all votes.&lt;/p>
&lt;p>Opposition parties show a trend of loosing on the left spectrum and gaining in the far right spectrum (&lt;code>AFD&lt;/code>) (see fig. 3 and 4).&lt;sup id="fnref1:12">&lt;a href="#fn:12" class="footnote-ref" role="doc-noteref">12&lt;/a>&lt;/sup>&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;p>Since the &lt;code>Linke&lt;/code> only wins less then minimum 5 percent in every poll, it is only listed by some polling institute.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;p>Both &lt;code>Linke&lt;/code> and &lt;code>FDP&lt;/code> did not meet the minimum of five percent. We therefore decided to omit the graphs.
Overall the fit of polls for each party is less than one percent of the election result.
The last polls before the election were conducted by &lt;code>Forschungsgruppe Wahlen&lt;/code> and &lt;code>INSA&lt;/code>.
&lt;code>Forschungsgruppe Wahlen&lt;/code> shows an average error per party of 0.87 percent points and &lt;code>INSA&lt;/code> 0.73 percentage points.
The larger error for &lt;code>Forschungsgruppe Wahlen&lt;/code> is caused as they did not add an estimation for &lt;code>Die Linke&lt;/code>.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;h2 id="posts">Posts&lt;/h2>
&lt;p>In the &lt;em>sub-sample&lt;/em> the cleaned posts with more than 50 characters in the evaluation data set contain 217 (median) and 248 ± 189 (average and standard deviation) characters. The maximum was above 5000 characters.
While in the &lt;em>full-sample&lt;/em> this changes to 281 ± 358 with a maximum of over 19000 characters and a median of 228 characters.
Since only German words were selected as search terms, the amount of English posts is very limited.&lt;/p>
&lt;p>We tried two different strategies for keeping posts that are attributable to single parties. In the first strategy we only keep posts that mention a single party, or their candidate(s). In the &lt;em>sub-sampled&lt;/em> we thus keep 39 % of all posts.
The second strategy keeps all posts, that mention a party more often than, all other parties combined. In the &lt;em>sub-sample&lt;/em> this kept 49 % of all posts. The connection between post and party is more clear with the first strategy, but we choose the second one, as more posts are preserved.
In addition, we changed the regional and party filter from the cleared posts to the raw posts. Cleaning removes, html-code. This includes also tags that are not mentioned in text. This way, we can still use the tags and keep 58 % percent of the posts of the &lt;em>sub-sample&lt;/em>. The aggressive strategy, to keep as many posts as possible, was used, because the &lt;code>Linke&lt;/code>, &lt;code>SPD&lt;/code>, &lt;code>Gruene&lt;/code>, and &lt;code>FDP&lt;/code> had a very low count of left posts in the &lt;em>sub-sample&lt;/em>, less than 10 posts per party. We do get the impression that this has also changed due to changes in the monitoring later on, which seams to have an retroactive effect for the smaller parties.
Of the &lt;em>sub-sample&lt;/em> dataset we thus label 8.1 % of posts as Bavarian, 6.14 % due the uses instance, 0.4 % due to a Bavarian location name in the user field and 2.0 % due to a Bavarian location name in the user notes. After filtering the &lt;em>sub-samples&lt;/em> contains 2647 toots of which we estimates 96 as Bavarian and 927 as other German.&lt;/p>
&lt;p>In the &lt;em>full-sample&lt;/em> 307 users (3.88 % percent) are from Bavaria: 2.68 % percent due to their Mastodon instance, 0.18 % due to their user fields and 1.18 % due to their user notes in total. In contrast, if we only consider posts that were kept after filtering, we keep 160 (8.9 %) Bavarian users and 1628 other German users. We thus kept 549 toots (10.0 %) as Bavarian and 4235 toots as other German, although more than 15 % of German inhabitants are Bavarian (see tab. 3 and 4).&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: left">Posts&lt;/th>
&lt;th style="text-align: right">Sub-Sample&lt;/th>
&lt;th style="text-align: right">Full Sample&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: left">&lt;strong>Total Count&lt;/strong>&lt;/td>
&lt;td style="text-align: right">4563&lt;/td>
&lt;td style="text-align: right">33142&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Topic: Bavaria&lt;/td>
&lt;td style="text-align: right">3363&lt;/td>
&lt;td style="text-align: right">10001&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">With dominant Party&lt;/td>
&lt;td style="text-align: right">2627&lt;/td>
&lt;td style="text-align: right">5763&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Bavarian Posts&lt;/td>
&lt;td style="text-align: right">249&lt;/td>
&lt;td style="text-align: right">549&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Other German Posts&lt;/td>
&lt;td style="text-align: right">2255&lt;/td>
&lt;td style="text-align: right">4921&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;!-- raw HTML omitted -->
&lt;p>After filtering for dominant parties, we still have posts in other languages. Finally, we filter posts that are older than the starting date of this study. After filtering these we get the count of Bavaria and other German regions.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: left">Users&lt;/th>
&lt;th style="text-align: right">Sub-Sample&lt;/th>
&lt;th style="text-align: right">Full Sample&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: left">&lt;strong>Total Count&lt;/strong>&lt;/td>
&lt;td style="text-align: right">1547&lt;/td>
&lt;td style="text-align: right">7918&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Thereof, Bavaria Users&lt;/td>
&lt;td style="text-align: right">126&lt;/td>
&lt;td style="text-align: right">307&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">German Users, After Filtering&lt;/td>
&lt;td style="text-align: right">1023&lt;/td>
&lt;td style="text-align: right">1788&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Thereof, Bavaria Users&lt;/td>
&lt;td style="text-align: right">96&lt;/td>
&lt;td style="text-align: right">160&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Thereof, Other German Users&lt;/td>
&lt;td style="text-align: right">927&lt;/td>
&lt;td style="text-align: right">1628&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Most posts (91.5 %) where self-labelled as German. Language classification increased this percentage to 97.7 %. Hence, about 6 % percent of the posts are relabelled. Most posts that are relabelled have been originally be set to the languages: &amp;ldquo;en&amp;rdquo;, unlabelled, and &amp;ldquo;en-us&amp;rdquo;. We assume that this are standard setting of the posting tools that have not been changed.&lt;/p>
&lt;p>The selected German and Bavarian posts where mainly posted during day time, with two peek times at noon and late afternoon (considering time zones). The time is recorded in UTC. While Sunday shows a higher frequency of tooting, the other weekdays share similar frequencies. The days with the highest frequencies are the 247th day of the year (Sep. 4th) and the election day (Oct 8th). In general we observe more posts during the Aiwanger affair, than around the election day. The frequencies kept decreasing after the election (see fig. 5).&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;h2 id="sentiments">Sentiments&lt;/h2>
&lt;p>In the &lt;em>sub-sample&lt;/em> only the parties &lt;code>FW&lt;/code>, &lt;code>CSU&lt;/code>, and &lt;code>AFD&lt;/code> were mentioned often to contain multiple posts for most days. This situation was enhanced by adding the search for data retrieval and the longer entry list of 40 posts. As a result we got more posts for the other parties and have several posts per day for the &lt;code>SPD&lt;/code>.
Fitting is done with a loess fit with a bandwidth of 0.5.
About 37 % of all toots mention the &lt;code>CSU&lt;/code> as main political topic. The average sentiment (loess fit) of the CSU ranges from -0.5 to -0.4. About 44 % percent of the posts&amp;rsquo; main topic is the &lt;code>FW&lt;/code>. The average sentiment (loess fit) for the FW is around -0.5. The &lt;code>AFD&lt;/code> was in twelve percent of all filtered posts. The loess fit of the sentiment is in the range of -0.4 (see fig. 6).&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;p>The count of mentioned parties over the full filtered sample is listed in tab. 5.
Beside the percentage based method, we also show the percentage
of all posts weighted by the count of followers of each author. Finally, we also show the percentage posts by Bavarian users.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Party&lt;/th>
&lt;th style="text-align: right">Percentage&lt;/th>
&lt;th style="text-align: right">Percentage Followers&lt;/th>
&lt;th style="text-align: right">Percentage Bavaria&lt;/th>
&lt;th style="text-align: right">Election Result&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>AFD&lt;/td>
&lt;td style="text-align: right">11.7 %&lt;/td>
&lt;td style="text-align: right">18.4 %&lt;/td>
&lt;td style="text-align: right">11.0 %&lt;/td>
&lt;td style="text-align: right">14.6 %&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>CSU&lt;/td>
&lt;td style="text-align: right">30.7 %&lt;/td>
&lt;td style="text-align: right">28.5 %&lt;/td>
&lt;td style="text-align: right">32.6 %&lt;/td>
&lt;td style="text-align: right">37.0 %&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>FDP&lt;/td>
&lt;td style="text-align: right">1.9 %&lt;/td>
&lt;td style="text-align: right">0.7 %&lt;/td>
&lt;td style="text-align: right">1.3 %&lt;/td>
&lt;td style="text-align: right">3.0 %&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>FW&lt;/td>
&lt;td style="text-align: right">47.7 %&lt;/td>
&lt;td style="text-align: right">45.3 %&lt;/td>
&lt;td style="text-align: right">49.1 %&lt;/td>
&lt;td style="text-align: right">15.8 %&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Gruene&lt;/td>
&lt;td style="text-align: right">3.0 %&lt;/td>
&lt;td style="text-align: right">1.2 %&lt;/td>
&lt;td style="text-align: right">1.8 %&lt;/td>
&lt;td style="text-align: right">14.4 %&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Linke&lt;/td>
&lt;td style="text-align: right">1.3 %&lt;/td>
&lt;td style="text-align: right">1.7 %&lt;/td>
&lt;td style="text-align: right">1.8 %&lt;/td>
&lt;td style="text-align: right">1.5 %&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SPD&lt;/td>
&lt;td style="text-align: right">3.7 %&lt;/td>
&lt;td style="text-align: right">4.1 %&lt;/td>
&lt;td style="text-align: right">2.1 %&lt;/td>
&lt;td style="text-align: right">8.4 %&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;!-- raw HTML omitted -->
&lt;p>These percentages are 30 percent points too high for &lt;code>Freie Waehler&lt;/code>.
Therefore, we estimate the same data again, but only within the time period from
day of the year 260 (Sep. 17th) to 280 (Oct. 7th), after the Aiwanger affair calmed down to the day before the election (see tab. 6).&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Party&lt;/th>
&lt;th style="text-align: right">Percentage&lt;/th>
&lt;th style="text-align: right">Percentage Followers&lt;/th>
&lt;th style="text-align: right">Percentage Bavaria&lt;/th>
&lt;th style="text-align: right">Election Result&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>AFD&lt;/td>
&lt;td style="text-align: right">20.6 %&lt;/td>
&lt;td style="text-align: right">46.9 %&lt;/td>
&lt;td style="text-align: right">17.3 %&lt;/td>
&lt;td style="text-align: right">14.6 %&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>CSU&lt;/td>
&lt;td style="text-align: right">45.3 %&lt;/td>
&lt;td style="text-align: right">30.5 %&lt;/td>
&lt;td style="text-align: right">42.7 %&lt;/td>
&lt;td style="text-align: right">37.0 %&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>FDP&lt;/td>
&lt;td style="text-align: right">1.3 %&lt;/td>
&lt;td style="text-align: right">0.3 %&lt;/td>
&lt;td style="text-align: right">0.9 %&lt;/td>
&lt;td style="text-align: right">3.0 %&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>FW&lt;/td>
&lt;td style="text-align: right">21.6 %&lt;/td>
&lt;td style="text-align: right">14.9 %&lt;/td>
&lt;td style="text-align: right">28.2 %&lt;/td>
&lt;td style="text-align: right">15.8 %&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Gruene&lt;/td>
&lt;td style="text-align: right">5.9 %&lt;/td>
&lt;td style="text-align: right">4.1 %&lt;/td>
&lt;td style="text-align: right">6.4 %&lt;/td>
&lt;td style="text-align: right">14.4 %&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Linke&lt;/td>
&lt;td style="text-align: right">0.7 %&lt;/td>
&lt;td style="text-align: right">0.0 %&lt;/td>
&lt;td style="text-align: right">0.9 %&lt;/td>
&lt;td style="text-align: right">1.5 %&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SPD&lt;/td>
&lt;td style="text-align: right">4.6 %&lt;/td>
&lt;td style="text-align: right">3.2 %&lt;/td>
&lt;td style="text-align: right">3.6 %&lt;/td>
&lt;td style="text-align: right">8.4 %&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;!-- raw HTML omitted -->
&lt;p>This increases the accuracy for the &lt;code>FW&lt;/code> and &lt;code>SPD&lt;/code>, but still strongly underestimates the &lt;code>Gruene&lt;/code> party.
Finally, we add the condition to only keep the most positive posts per user. This is closer to a voting intent. We still overestimate the &lt;code>CSU&lt;/code> and &lt;code>FW&lt;/code>. All this filtering results in very small supports for Bavarian authors (see tab. 7).&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Party&lt;/th>
&lt;th style="text-align: right">Percentage&lt;/th>
&lt;th style="text-align: right">Percentage Followers&lt;/th>
&lt;th style="text-align: right">Percentage Bavaria&lt;/th>
&lt;th style="text-align: right">Election Result&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>AFD&lt;/td>
&lt;td style="text-align: right">18.7 %&lt;/td>
&lt;td style="text-align: right">43.2 %&lt;/td>
&lt;td style="text-align: right">16.1 %&lt;/td>
&lt;td style="text-align: right">14.6 %&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>CSU&lt;/td>
&lt;td style="text-align: right">45.9 %&lt;/td>
&lt;td style="text-align: right">31.1 %&lt;/td>
&lt;td style="text-align: right">38.1 %&lt;/td>
&lt;td style="text-align: right">37.0 %&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>FDP&lt;/td>
&lt;td style="text-align: right">1.0 %&lt;/td>
&lt;td style="text-align: right">0.7 %&lt;/td>
&lt;td style="text-align: right">-&lt;/td>
&lt;td style="text-align: right">3.0 %&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>FW&lt;/td>
&lt;td style="text-align: right">22.0 %&lt;/td>
&lt;td style="text-align: right">14.1 %&lt;/td>
&lt;td style="text-align: right">30.6 %&lt;/td>
&lt;td style="text-align: right">15.8 %&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Gruene&lt;/td>
&lt;td style="text-align: right">6.4 %&lt;/td>
&lt;td style="text-align: right">2.0 %&lt;/td>
&lt;td style="text-align: right">8.1 %&lt;/td>
&lt;td style="text-align: right">14.4 %&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Linke&lt;/td>
&lt;td style="text-align: right">1.4 %&lt;/td>
&lt;td style="text-align: right">0.3 %&lt;/td>
&lt;td style="text-align: right">1.6 %&lt;/td>
&lt;td style="text-align: right">1.5 %&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SPD&lt;/td>
&lt;td style="text-align: right">4.6 %&lt;/td>
&lt;td style="text-align: right">8.8 %&lt;/td>
&lt;td style="text-align: right">4.8 %&lt;/td>
&lt;td style="text-align: right">8.4 %&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>After the additional filtering, the &lt;code>FDP&lt;/code> was not mentioned by Bavarian users.&lt;/p>
&lt;p>In addition, we estimate the frequencies, weighted by the follower count of its authors. This results in a strong overestimation of the &lt;code>AFD&lt;/code>. The average error per party is well above the other methods tested here.
Removing data before the Aiwanger affair reduced the average error per party from the range of 8 to 9 percentage points, to 5 percentage points. Using the most positive sentiment per author again reduced the error to 4.4 (Bavaria) and to 4.7 (other German regions) percentage points. In the election result, the mentioned parties only add to 94.7 % of all valid votes, due to smaller parties we did not monitor. If we correct our prediction by the 94.7 %,
we do not get an decreased overall error for the posts in tab. 5. But in the datasets that exclude the period of the Aiwanger affair (tab. 6 and 7), we decrease the average error by 10 to 12 %. Hence our best prediction was increased from an average error of 4.4 percentage points, to 4.o percentage points.&lt;/p>
&lt;h2 id="compare-sentiments-and-polls">Compare Sentiments and Polls&lt;/h2>
&lt;p>The sentiment is shifted from the range -1 to 1 to the range 0 to 1 (see formula (1)). The sentiments of the posts were aggregated for the same day or calendar week, with the mean function. Missing values are filled forward first and than backward.&lt;/p>
&lt;p>$$
S_n(\text{party}) = \frac{S(\text{party}) + 1}{2} \tag{1}
$$&lt;/p>
&lt;p>Then the sentiment is weighted by the sum of the sentiment of all parties for the same week (see formula (2)). This is used as a measure to correct the sentiment by how well the sentiment of other parties is in the same time period. If comparing the daily aggregate and the weekly aggregate, we can see that the variance is much higher for the sentiment compared to the polls. This is strongly reduced by the weekly aggregates (see fig 8.)&lt;/p>
&lt;p>$$
S_p (party) = \frac{S_n(party)}{∑_{party} S_n} \tag{2}
$$&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;p>We would underestimate the result of the &lt;code>CSU&lt;/code> at any given point in time. The result for the &lt;code>AFD&lt;/code> is relative close, while we overestimate smaller parties (see fig. 9).&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;p>The graph that compares the polling result to the sentiments (fig. 10), shows that the main dependency is the party itself.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;p>The offset of the cross correlation for the timelines for all parties is zero.&lt;/p>
&lt;p>The support of Bavarian samples, is already small. We therefore only group by calendar week, or the region. Group by both would reduce the support again thus increase the error for small parties.
For the calendar week 40 (week of the election) the sentiment based election prediction is shown below. This approach strongly overestimates smaller parties. The predictions of all parties are much closer to each other. In addition, to the bare sentiment for calendar week 40, we apply a linear fit between the polls and the weekly sentiment $ S_p $ for the last six election weeks.
The parties are one-hot encoded. Which means each Party is an attribute. This Attributes encodes, whether this post is about the party as 1 (true) or 0 (false). The poll result depend much stronger on the party itself than on the weekly averaged sentiment. The slope for the sentiment is only 0.024. Which means that in the model the sentiment only accounts for a maximum of 2.4 percentage points. Everything else is depending on the party itself. R²-score on the test dataset is 0.997.
In addition, we show the result of the fit for a ridge regression with &lt;code>alpha&lt;/code>=0.1. The fits for calendar week 40 is shown in the table below.
The grid search for different &lt;code>alpha&lt;/code> showed the best result for &lt;code>alpha&lt;/code> = 0, which is a normal linear fit.
The ridge regression regularizes higher weights and thus increases smaller weights to reduce the error. This reduces the weights for the larger parties and increases the weights for smaller parties. In addition the influence of the sentiment was increased to 9.9 percentage points. This meanly increases the error if the linear fit already underestimates the election results, and decreases it if the result was overestimated by the linear fit (see tab. 8).
The average error for the linear fitted result is 0.40 percentage points and 0.59 % for the ridge regression. This is less than the error of the last polls
of &lt;code>Forschungsgruppe Wahlen&lt;/code> and &lt;code>INSA&lt;/code>. The error of the linear fit is about the same size as average error over all polls of the last six weeks, which is 0.39.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Party&lt;/th>
&lt;th style="text-align: right">Sentiment KW 40&lt;/th>
&lt;th style="text-align: right">Average Polls&lt;/th>
&lt;th style="text-align: right">Linear Fit KW 40&lt;/th>
&lt;th style="text-align: right">Ridge Fit KW 40&lt;/th>
&lt;th style="text-align: right">Election Result&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>AFD&lt;/td>
&lt;td style="text-align: right">15.8&lt;/td>
&lt;td style="text-align: right">13.9 %&lt;/td>
&lt;td style="text-align: right">14.1 %&lt;/td>
&lt;td style="text-align: right">13.7 %&lt;/td>
&lt;td style="text-align: right">14.6 %&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>CSU&lt;/td>
&lt;td style="text-align: right">15.4&lt;/td>
&lt;td style="text-align: right">36.4 %&lt;/td>
&lt;td style="text-align: right">36.4 %&lt;/td>
&lt;td style="text-align: right">35.7 %&lt;/td>
&lt;td style="text-align: right">37.0 %&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>FDP&lt;/td>
&lt;td style="text-align: right">12.7&lt;/td>
&lt;td style="text-align: right">3.4 %&lt;/td>
&lt;td style="text-align: right">3.4 %&lt;/td>
&lt;td style="text-align: right">3.2 %&lt;/td>
&lt;td style="text-align: right">3.0 %&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>FW&lt;/td>
&lt;td style="text-align: right">13.7&lt;/td>
&lt;td style="text-align: right">16.0 %&lt;/td>
&lt;td style="text-align: right">16.2 %&lt;/td>
&lt;td style="text-align: right">15.1 %&lt;/td>
&lt;td style="text-align: right">15.8 %&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Gruene&lt;/td>
&lt;td style="text-align: right">19.2&lt;/td>
&lt;td style="text-align: right">14.6 %&lt;/td>
&lt;td style="text-align: right">14.6 %&lt;/td>
&lt;td style="text-align: right">14.5 %&lt;/td>
&lt;td style="text-align: right">14.4 %&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Linke&lt;/td>
&lt;td style="text-align: right">8.4&lt;/td>
&lt;td style="text-align: right">1.4 %&lt;/td>
&lt;td style="text-align: right">1.4 %&lt;/td>
&lt;td style="text-align: right">1.0 %&lt;/td>
&lt;td style="text-align: right">1.5 %&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SPD&lt;/td>
&lt;td style="text-align: right">14.7&lt;/td>
&lt;td style="text-align: right">9.0 %&lt;/td>
&lt;td style="text-align: right">9.0 %&lt;/td>
&lt;td style="text-align: right">8.9%&lt;/td>
&lt;td style="text-align: right">8.4 %&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h1 id="discussion">Discussion&lt;/h1>
&lt;p>Each poll has at least a sample size of 1000 people. In contrast,we monitored 4921 posts that are from other regions of Germany, from 1728 users. But from Bavaria we only got 549 posts, written by 160 users, which results very likely in an unrepresentative sampling.
Most toots about the &lt;code>FW&lt;/code> are made at the start of the sampling period when the Aiwanger affair was the main topic of the election.&lt;sup id="fnref1:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>
The number of posts we evaluated is very low. The ability to distinguish the Bavarian posts from other German posts is very limited. The differences that are shown in the frequencies thus have limited information value.&lt;/p>
&lt;p>The changes in polls results have to be larger than the resolution and error of the polls themselves. The resolution given by the source is 1 %. The errors of polls are not given by each party, they are often given as two percentage points for parties that reach 10 % and an error of three percentage points, at 50 %.&lt;sup id="fnref:21">&lt;a href="#fn:21" class="footnote-ref" role="doc-noteref">21&lt;/a>&lt;/sup> Because, the changes in the pools are slow, a longer time frame is needed to accumulate changes that are above this resolution or error.&lt;/p>
&lt;p>Daily sentiment analysis shows more noise than poll results. Weekly or monthly averages have to be used, due to the higher number of samples on longer time frames, but also due to the fact that sentiments are not voting intentions. While daily changes in politics might show strong reactions in sentiment, we assume that voting intentions might only shift on a longer time period.&lt;/p>
&lt;p>We have a lower count of samples for the parties &lt;code>SPD&lt;/code>, &lt;code>Buendis 90/ Gruene&lt;/code>, and &lt;code>FDP&lt;/code> that govern at the federal level. Thus, sentiment analysis on a national scale is more promising.&lt;/p>
&lt;p>We tried to predict election result with two different versions:&lt;/p>
&lt;ol>
&lt;li>Frequency based,&lt;/li>
&lt;li>Sentiment based.&lt;/li>
&lt;/ol>
&lt;p>The frequency based version favoured the &lt;code>FW&lt;/code> strongly above their election result by about 30 percentage points. After removing all mentions in the period of the Aiwanger affair the error could be greatly reduced. We still strongly favour the &lt;code>FW&lt;/code>, 14 to 20 percentage points above their potential. Probably due to the strongly local factor of the party. We assume that it is less overshadowed by national politics as other parties.&lt;/p>
&lt;p>If we compare the error of the predicted election result to the actual election result, we get an average error of four to nine percentage points per party. The best prediction is made by most positive post of Bavarian authors recorded between Sep. 17th and Oct. 7th. for the highest sentiment per user with an average error of 4.4 percentage points. This is done to emulate the voting intent. We assume that the voting intent can be better emulated if we filter out negative sentiment.&lt;/p>
&lt;p>Removing the time period of the Aiwanger affair reduces the average error from 9 to 5 percentage points. Comparing the posts of Bavarians to other German authors, we only slightly decrease the error.&lt;/p>
&lt;p>Considering the sentiment based method we can see that the weekly sentiment for the parties &lt;code>CSU&lt;/code>, &lt;code>FW&lt;/code> and &lt;code>AFD&lt;/code> is very similar. This is also true for the other parties like &lt;code>SPD&lt;/code>. The sentiment for the &lt;code>Buendis 90 / Gruene&lt;/code> is slightly higher.
Filtering by the most positively mentioned party per author, reduces the error slightly. We assume that while not all parties are mentioned by each user, the true vote intent is still partially hidden. Therefore, we might select the least negatively mentioned party instead.&lt;/p>
&lt;p>The roberta base language detection model lists a F1-score of 0.967 for German and 1.000 for English.&lt;sup id="fnref1:15">&lt;a href="#fn:15" class="footnote-ref" role="doc-noteref">15&lt;/a>&lt;/sup> The German language Sentiment model lists an overall F1-micro score of 0.9639.&lt;sup id="fnref1:16">&lt;a href="#fn:16" class="footnote-ref" role="doc-noteref">16&lt;/a>&lt;/sup> The English language Sentiment model lists an F1-score of 0.72.&lt;sup id="fnref1:18">&lt;a href="#fn:18" class="footnote-ref" role="doc-noteref">18&lt;/a>&lt;/sup>
Still, the models can not detect sarcasm, as this needs a lot of background knowledge. This is considered a different task in the field of text mining.&lt;sup id="fnref2:18">&lt;a href="#fn:18" class="footnote-ref" role="doc-noteref">18&lt;/a>&lt;/sup>&lt;/p>
&lt;p>About 10 % of all kept posts were labelled as Bavarian. We do not know whether this is due to lower number of Bavarian people who use Mastodon or whether they use generalised instances, so that we could not distinguish them as Bavarian users. We tried to mitigate this problem with the self-disclosure in user fields and user texts. We did not try to classify the users by their language usage.&lt;/p>
&lt;p>The best result was based on the polling data. The average of the polls of the last six weeks before the election resulted into very small error of 0.39 percentage points per party.
Similar results could be achieved by a fit of the polls of each party versus the sentiment analysis. The dependency of the poll result from the sentiment was small. It has to be tested whether this effect can be repeated for other elections.&lt;/p>
&lt;h1 id="conclusion">Conclusion&lt;/h1>
&lt;p>The frequency analysis has been enhanced by filtering early and late posts and only keeping the most positive post per user. This reduced the error to 4.4 percentage points. To better emulate voting intent, we would need to further filter out negative posts. We propose to reduce the error by finding the best cut-of at which sentiments are considered too negative.&lt;/p>
&lt;p>We see strong problem for using Mastodon as a basis for polling (regional) elections.
We assume that for national elections Mastodon might be a better data source.
The number of toots in the Bavarian state election 2023 was relatively low. We only got a high number of posts during the Aiwanger affair, which in turn added a bias to the frequencies for all parties.&lt;/p>
&lt;p>In addition, we assume that the user base is not representative for the socio-demographics of Germany or any German state. Wherefore, we propose a two layered
strategy to mitigate this by applying a weighted average of the user data.
We propose that gender can be extracted from user data like user name, user fields and user texts. For users that do not share this information we propose an image classification.
Similar we propose an image classification to retrieve the age of the users. For images with no person or multiple persons, we assume a random gender and a random age (drawn from the image/user distribution). Some models for the age a gender classification are listed at &lt;a href="https://paperswithcode.com/sota/age-and-gender-classification-on-adience">paperswithcode&lt;/a> and also in the &lt;a href="https://github.com/onnx/models/tree/main/vision/body_analysis/age_gender">ONNX model zoo&lt;/a>.
This would enable us to compare the user demographics of sample to the Bavarian Census.&lt;sup id="fnref:22">&lt;a href="#fn:22" class="footnote-ref" role="doc-noteref">22&lt;/a>&lt;/sup>&lt;/p>
&lt;h1 id="appendix">Appendix&lt;/h1>
&lt;h2 id="bavaria-demographics">Bavaria Demographics&lt;/h2>
&lt;p>Bavaria has about 13.3 M and slightly more women (50.5 %).&lt;sup id="fnref1:22">&lt;a href="#fn:22" class="footnote-ref" role="doc-noteref">22&lt;/a>&lt;/sup>
This is due to the age distribution of its citizen, caused by a higher mortality rate of men at higher ages. The peer group aged 40 to 50 is the first one with more than 50 % women.
All younger peer groups show an surplus of men by two to five percent. On the other hand, the peer group of age 75 or older shows a surplus of women of about 17 %.&lt;/p>
&lt;h2 id="demographics-sample">Demographics Sample&lt;/h2>
&lt;p>In the sub-sample we encountered following keys in the user fields that correspondent with the age of the user, listed in tab. 9.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Category&lt;/th>
&lt;th>Keywords&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Age&lt;/td>
&lt;td>age, Alter&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Birth&lt;/td>
&lt;td>born, Geburtstag&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>We encounter keys that describe the gender/sex are listed tab 10. by their group.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Category&lt;/th>
&lt;th>Keywords&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Gender&lt;/td>
&lt;td>Gender, Geschlecht, Sexualität&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Pronouns&lt;/td>
&lt;td>Pronom, Pronomen, Pronoun, Pronouns, Pronomina, Pronoms&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Other&lt;/td>
&lt;td>Wer&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>The values for the user fields concerning gender/sexuality are grouped by sex in tab. 11.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Category&lt;/th>
&lt;th>Values&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Male&lt;/td>
&lt;td>he, him, his, er, ihm, ihn, sein&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Female&lt;/td>
&lt;td>she, her, sie, ihr&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;!-- raw HTML omitted -->
&lt;h2 id="list-of-bavarian-instances">List of Bavarian Instances&lt;/h2>
&lt;p>The lists of Bavarian instances has been adapted from the openstreetmap &lt;code>Mastodon Near me&lt;/code>.&lt;sup id="fnref2:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup> The instances are grouped and listed in tab. 12.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Category&lt;/th>
&lt;th>Values&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Cities&lt;/td>
&lt;td>muenchen.social, augsburg.social, nuernberg.social, wue.social, ploen.social, mastodon.dachgau.social&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Other&lt;/td>
&lt;td>mastodon.bayern, sueden.social&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>wue.social is the instance for the city of Wuerzburg. sueden.social is a more general instance for southern Germany, but we assume that the overlapping and regional proximity lead to a similar sentiment.&lt;/p>
&lt;h2 id="list-of-keywords-in-table-fields-that-transcode-locations">List of Keywords in Table Fields that Transcode Locations&lt;/h2>
&lt;p>Some users user there user fields, to contain some information about there location. The user fields keys that are used to filter are listed by there category in tab. 13.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Category&lt;/th>
&lt;th>Values&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Home&lt;/td>
&lt;td>&amp;ldquo;heimat&amp;rdquo;, &amp;ldquo;heimathafen&amp;rdquo;, &amp;ldquo;heimatort&amp;rdquo;, &amp;ldquo;home&amp;rdquo;, &amp;ldquo;wohnhaft&amp;rdquo;, &amp;ldquo;wohnort&amp;rdquo;, &amp;ldquo;wohnt in&amp;rdquo;, &amp;ldquo;zuhause&amp;rdquo;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Birthplace&lt;/td>
&lt;td>&amp;ldquo;born where&amp;rdquo;, &amp;ldquo;herkunft&amp;rdquo;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Region&lt;/td>
&lt;td>&amp;ldquo;bundesland&amp;rdquo;, &amp;ldquo;country&amp;rdquo;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Other&lt;/td>
&lt;td>&amp;ldquo;adresse&amp;rdquo;, &amp;ldquo;city&amp;rdquo;, &amp;ldquo;location&amp;rdquo;, &amp;ldquo;ort&amp;rdquo;, &amp;ldquo;standort&amp;rdquo;, &amp;ldquo;wahlkreis&amp;rdquo;, &amp;ldquo;wo&amp;rdquo;, &amp;ldquo;📍&amp;rdquo;&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="monitoring-interruptions">Monitoring Interruptions&lt;/h2>
&lt;p>The monitoring service was interrupted on Sep. 30th 2023 for about 20 hours and on Oct. 7th 2023 for the update. A longer interruption took place from Oct. 28th to 31th and lasted about 85 hours.&lt;/p>
&lt;h2 id="tech-stack">Tech stack&lt;/h2>
&lt;p>Saving Mastodon data into a SQLite database is done with an Elixir project with mix.
The Mastodon api is called with &lt;a href="https://hexdocs.pm/httpoison/readme.html">HTTPoison&lt;/a>. &lt;a href="https://hexdocs.pm/ecto/Ecto.html">Ecto&lt;/a> applied to write the reply into a SQLite3 database. Cron is utilised to hourly trigger the api calls with &lt;a href="https://hexdocs.pm/quantum/readme.html">Quantum&lt;/a>.&lt;/p>
&lt;p>The data evaluation is done with the &lt;a href="https://hexdocs.pm/nx/Nx.html">Nx&lt;/a> framework. The first version of Nx was released 01/06/2022. Nx enables the numerical computation in Elixir and is used for Machine Learning (&lt;a href="https://hexdocs.pm/scholar/Scholar.html">Scholar&lt;/a>) and Deep Learning libraries (&lt;a href="https://hexdocs.pm/axon/Axon.html">Axon&lt;/a>), for instance with the &lt;a href="https://hexdocs.pm/exla/EXLA.html">EXLA&lt;/a> compiler as backed.
The packages are still in development and sometimes new functions and fixes were just released during the project. For instance the &lt;a href="https://github.com/elixir-explorer/explorer/issues/714">week_of_year&lt;/a> function was created for this.&lt;/p>
&lt;h1 id="references">References&lt;/h1>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>Rochko, Eugen: &lt;em>Annual Report 2022&lt;/em>, &lt;a href="https://blog.joinmastodon.org/2023/10/annual-report-2022/">https://blog.joinmastodon.org/2023/10/annual-report-2022/&lt;/a>, Version: Dec. 10 2023.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>&lt;em>ActivityPub&lt;/em>, &lt;a href="https://en.wikipedia.org/wiki/ActivityPub">https://en.wikipedia.org/wiki/ActivityPub&lt;/a>, Version: Dec. 10 2023.&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>&lt;em>FediDB - Fediverse Network Statistics&lt;/em>, &lt;a href="https://fedidb.org/">https://fedidb.org/&lt;/a>, Version: Dec. 10 2023.&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4">
&lt;p>&lt;em>Mastodon Users (@mastodonusercount@mastodon.social)&lt;/em>,&lt;a href="https://mastodon.social/@mastodonusercount">https://mastodon.social/@mastodonusercount&lt;/a>, Version: Dec. 10 2023.&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5">
&lt;p>&lt;em>Hubert Aiwanger&lt;/em>, &lt;a href="https://en.wikipedia.org/wiki/Hubert_Aiwanger">https://en.wikipedia.org/wiki/Hubert_Aiwanger&lt;/a>, Wikipedia, : Dec. 10 2023.&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&amp;#160;&lt;a href="#fnref1:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6">
&lt;p>Auer; Katja, Beck; Sebastian: Glas; Andreas, Ott; Klaus: &lt;em>Hubert Aiwanger soll als Schüler ein antisemitisches Flugblatt verfasst haben&lt;/em>, Süddeutsche.de, Version: Dec. 15 2023.&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7">
&lt;p>Koch, Wolfgang: &lt;em>Ergebnisse der ARD/ZDF-Onlinestudie 2023&lt;/em>, &lt;a href="https://www.ard-zdf-onlinestudie.de/files/2023/MP_26_2023_Onlinestudie_2023_Social_Media.pdf">https://www.ard-zdf-onlinestudie.de/files/2023/MP_26_2023_Onlinestudie_2023_Social_Media.pdf&lt;/a>, Version: Dec. 10 2023.&amp;#160;&lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8">
&lt;p>&lt;em>Mastodon Near Me - Global Mastodon server list by country and region - uMap&lt;/em>, &lt;a href="https://umap.openstreetmap.fr/en/map/mastodon-near-me-global-mastodon-server-list-by-co_828094">https://umap.openstreetmap.fr/en/map/mastodon-near-me-global-mastodon-server-list-by-co_828094&lt;/a>, Version: Dec. 10 2023.&amp;#160;&lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&amp;#160;&lt;a href="#fnref1:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&amp;#160;&lt;a href="#fnref2:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:9">
&lt;p>&lt;em>Find where to sign up for the decentralized social network Mastodon.&lt;/em>, &lt;a href="https://joinmastodon.org/servers">https://joinmastodon.org/servers&lt;/a>, Version: Dec. 10 2023.&amp;#160;&lt;a href="#fnref:9" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:10">
&lt;p>&lt;em>Sentiment analysis&lt;/em>, &lt;a href="https://en.m.wikipedia.org/wiki/Sentiment_analysis">https://en.m.wikipedia.org/wiki/Sentiment_analysis&lt;/a>, Wikipedia, : Dec. 15 2023.&amp;#160;&lt;a href="#fnref:10" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:11">
&lt;p>Pekar, Viktor; Najafi, Hossein, Binner, Jane M., Swanson, Ridley; Rickard, Charles, Fry, John: Voting intentions on social media and political opinion polls, &lt;em>Government Information Quarterly&lt;/em> 39 (2022), 1-50.&amp;#160;&lt;a href="#fnref:11" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:12">
&lt;p>Bohrn, Brandon: &lt;em>The Evolution of Germany’s Political Spectrum | Politics &amp;amp; Society&lt;/em>, &lt;a href="https://bfna.org/politics-society/the-evolution-of-germanys-political-spectrum-19esvea4sk/">https://bfna.org/politics-society/the-evolution-of-germanys-political-spectrum-19esvea4sk/&lt;/a>, Version: Dec. 13 2023.&amp;#160;&lt;a href="#fnref:12" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&amp;#160;&lt;a href="#fnref1:12" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:13">
&lt;p>&lt;em>Landtagswahl in Bayern 2023: Kandidaten, Themen, Termin&lt;/em>, &lt;a href="https://www.br.de/nachrichten/bayern/landtagswahl-in-bayern-2023-termin-themen-kandidaten,TMD4uSM">https://www.br.de/nachrichten/bayern/landtagswahl-in-bayern-2023-termin-themen-kandidaten,TMD4uSM&lt;/a>, Version: Dec. 10 2023.
&lt;em>Bayerische Linke kürt Adelheid Rupp als Spitzenkandidatin&lt;/em>&lt;a href="https://www.br.de/nachrichten/bayern/bayerische-linke-kuert-adelheid-rupp-als-spitzenkandidatin,TZXl5yd">https://www.br.de/nachrichten/bayern/bayerische-linke-kuert-adelheid-rupp-als-spitzenkandidatin,TZXl5yd&lt;/a>, Version: Dec. 10 2023.&amp;#160;&lt;a href="#fnref:13" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:14">
&lt;p>&lt;em>OpenData&lt;/em>, &lt;a href="https://geodaten.bayern.de/opengeodata/OpenDataDetail.html?pn=verwaltung">https://geodaten.bayern.de/opengeodata/OpenDataDetail.html?pn=verwaltung&lt;/a>, Version: Dec. 10 2023.&amp;#160;&lt;a href="#fnref:14" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:15">
&lt;p>&lt;em>papluca/xlm-roberta-base-language-detection · Hugging Face&lt;/em>, &lt;a href="https://huggingface.co/papluca/xlm-roberta-base-language-detection">https://huggingface.co/papluca/xlm-roberta-base-language-detection&lt;/a>, Version: Dec. 10 2023.&amp;#160;&lt;a href="#fnref:15" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&amp;#160;&lt;a href="#fnref1:15" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:16">
&lt;p>&lt;em>oliverguhr/german-sentiment-bert · Hugging Face&lt;/em>, &lt;a href="https://huggingface.co/oliverguhr/german-sentiment-bert">https://huggingface.co/oliverguhr/german-sentiment-bert&lt;/a>, Version: Dec. 10 2023.&amp;#160;&lt;a href="#fnref:16" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&amp;#160;&lt;a href="#fnref1:16" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:17">
&lt;p>&lt;em>Tokenizer&lt;/em>, &lt;a href="https://platform.openai.com/tokenizer">https://platform.openai.com/tokenizer&lt;/a>, Version: Dec. 10 2023.&amp;#160;&lt;a href="#fnref:17" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:18">
&lt;p>&lt;em>finiteautomata/bertweet-base-sentiment-analysis · Hugging Face&lt;/em>, &lt;a href="https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis">https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis&lt;/a>, Version: Dec. 10 2023.
Pérez, Juan Manue;, Rajngewerc Mariela; Giudici, Juan Carlos; Furman, Damián A.; Luque, Franco; Alemany, Laura Alonso; Martínez, María Vanina: &lt;em>pysentimiento: A Python Toolkit for Opinion Mining and Social NLP tasks&lt;/em>,&lt;a href="https://arxiv.org/abs/2106.09462">https://arxiv.org/abs/2106.09462&lt;/a>, Version: Dec. 10 2023.&amp;#160;&lt;a href="#fnref:18" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&amp;#160;&lt;a href="#fnref1:18" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&amp;#160;&lt;a href="#fnref2:18" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:19">
&lt;p>Zhang, Shiwei; Zhang, Xiuzhen; Chan, Jeffrey; Rosso, Paolo: Irony detection via sentiment-based transfer learning, &lt;em>Information Processing &amp;amp; Management&lt;/em> 56 (2019), 1633-1644.&amp;#160;&lt;a href="#fnref:19" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:20">
&lt;p>&lt;em>Sonntagsfrage zur Landtagswahl 2023 in Bayern&lt;/em>, &lt;a href="https://www.wahlrecht.de/umfragen/landtage/bayern.htm#fn-bp">https://www.wahlrecht.de/umfragen/landtage/bayern.htm#fn-bp&lt;/a>}, Version: Dec. 10 2023.&amp;#160;&lt;a href="#fnref:20" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:21">
&lt;p>&lt;em>ARD-DeutschlandTrend: Zufriedenheit mit Kanzler Scholz auf Rekordtief&lt;/em>, &lt;a href="https://www.tagesschau.de/inland/deutschlandtrend/deutschlandtrend-3410.html">https://www.tagesschau.de/inland/deutschlandtrend/deutschlandtrend-3410.html&lt;/a>, Version: Dec. 12 2023.&amp;#160;&lt;a href="#fnref:21" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:22">
&lt;p>&lt;em>Bevölkerung in den Gemeinden Bayerns nach Altersgruppen und Geschlecht&lt;/em>, &lt;a href="https://www.statistik.bayern.de/mam/produkte/veroffentlichungen/statistische_berichte/a1310c_202200.pdf">https://www.statistik.bayern.de/mam/produkte/veroffentlichungen/statistische_berichte/a1310c_202200.pdf&lt;/a>, Bayerisches Landesamt für Statistik, Version: Dec. 10 2023.&amp;#160;&lt;a href="#fnref:22" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&amp;#160;&lt;a href="#fnref1:22" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div>
- https://sehheiden.github.io/posts/mastodon_on_bavarian_elecvtion/ -</description></item><item><title>Least Cost Path for Optimisation on Paths for Power Cables in North Germany</title><link>https://sehheiden.github.io/posts/cableroute/</link><pubDate>Wed, 15 Mar 2023 00:00:00 +0000</pubDate><guid>https://sehheiden.github.io/posts/cableroute/</guid><description>SeHe's Blog https://sehheiden.github.io/posts/cableroute/ -&lt;h1 id="sec:introduction">Introduction and Objectives&lt;/h1>
&lt;p>The German power system is adding renewable energy sources in the north,
where wind energy plants reach their highest efficiency, due to higher
wind speeds. At the same time old power plants e.g. nuclear, hard coal
and lignite are being phased out &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. These
older power plants were mainly located in southern and central Germany.
The energy sink, industrial and private demand, is not shifting north.
Therefore, the renewable energy has to be transported from north to
south which increases the congestion in the power grid. The amount of
offshore wind power, that the German energy system can use, can be
greatly increased by adding new power lines &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>However, it is not enough to find the shortest route when building new
power lines. Other parameters have to be taken into consideration, as
the steepness of a road or the soil for example, play an important role
for the building cost of a road or pipeline &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>.
When planing the additional routes for a power grid, further aspects
such as legal regulations and acceptance by the local population have to
kept in mind. Also technical aspects, as the effects on the grid
stability are further points to take into
consideration &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>Due to the increasing demand for renewable energy wind offshore wind
turbines supplied 5.5 % annual percentage in 2020 of the German energy
mix &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>The physical modelling in the power systems models ranges from not
modelling the grid at all to models that use of Kirchhoffs
laws &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. The calculation of a power system is
complex, because changing one edge changes the flow in all parts of
grid. Besides, the actual grid data are
confidential &lt;sup id="fnref1:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. Some models not only
consider the grid of one nation, but consider neighbouring states or the
whole European power system &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>. Older power grid
models are e.g. optimising the well fare, but economic modelling is not
sufficient for planning modern routes, as the aspect of environmental
sustainability, security of supply and the public acceptance play an
increasingly important role &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup> in modern
planning.&lt;/p>
&lt;p>In contrast to a GIS analysis, these other factors can be studied done
with a Multiple-criteria decision analysis (MCDA) &lt;sup id="fnref2:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>.
Stakeholders as decision makers can be included and combined with an
expert system &lt;sup id="fnref3:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>The uncertainties associated with MCDA are data uncertainties, preferential
uncertainties and model uncertainties are investigated with a
sensitivity analysis &lt;sup id="fnref4:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup> or
simulation &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>. Both inter- and intra-criteria
preferential uncertainties &lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup> can be
considered.&lt;/p>
&lt;p>This paper aims to find potential paths for power lines by using the
Least Cost Path algorithm. Land usage and planning data are used to
estimate the costs arising by using the local area. The task was to
provide the Least Cost Path using a web programming service (wps).
Beyond that, methods to reduce the needed
compute power for finding the Least Cost Path had to be studied.&lt;/p>
&lt;h1 id="sec:methods">Methods&lt;/h1>
&lt;p>The Least Cost Path algorithm is used to plan a potential,
cost-optimised route for a power line. The Least Cost Path algorithm is
a Dijkstra algorithm &lt;sup id="fnref:11">&lt;a href="#fn:11" class="footnote-ref" role="doc-noteref">11&lt;/a>&lt;/sup> applied on a raster map. The
vertices of the graph are the pixel centres, that are connected to the
eight neighbouring pixel centres via the edges. This makes it possible
to find routes on graphs that are not predefined, such as road networks.
The weights of the edges are the local costs of transit from one pixel
center to the neighbouring pixel center. The costs can be physical
costs, such as the local slope, but also can be composed of other
factors as the acceptance rate to transverse a given land usage. The
Least Cost Path algorithm consists of at least two successive steps. 1)
The first step is to aggregate the costs of travelling from the starting
point to a given set of end points. This step generates the aggregated
cost raster of travelling from the starting point to any point of the
cost raster. 2) In the next step the backtracking, the route of the
actual Least Cost Path is calculated. For each end point the path via
the lowest cost neighbour is taken until the starting point is reached.&lt;/p>
&lt;p>Some implementations switch the roles of start and ending points, so
that either many start points and one single end point, or one single
end point and many end points can be used. In some implementations there
is an extra step between 1) and 2) that calculates cost-weight direction
raster, that encodes the direction of the shortest path to the starting
point as integer values.&lt;/p>
&lt;p>We retrieve a set of different spacial data-sets from public sources as
a basis for creating the cost raster. The study area are the counties of
Cuxhaven and Osterholz in the state of Lower Saxony, Germany. Areas
protected by different European and National conservation laws are
provided by the German Environment Agency as Web Feature Service (wfs) &lt;sup id="fnref:12">&lt;a href="#fn:12" class="footnote-ref" role="doc-noteref">12&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>The national land coverage (ATKIS) with a scale of 1:250000 are provided
by the Federal Agency for Cartography and
Geodesy &lt;sup id="fnref:13">&lt;a href="#fn:13" class="footnote-ref" role="doc-noteref">13&lt;/a>&lt;/sup>. The national power grid (tags:
&amp;lsquo;power&amp;rsquo;: line) has been retrieved via
OpenStreetMap &lt;sup id="fnref:14">&lt;a href="#fn:14" class="footnote-ref" role="doc-noteref">14&lt;/a>&lt;/sup>. Local data as houses at Level of
Detail 1 are provided by the State Office for Geoinformation and Land
Surveying of Lower Saxony &lt;sup id="fnref:15">&lt;a href="#fn:15" class="footnote-ref" role="doc-noteref">15&lt;/a>&lt;/sup>. In addition,
local planning geodata for the land use are taken from &amp;lsquo;Metropolplaner&amp;rsquo;
(Planning data Lower Saxony &amp;amp; Bremen) &lt;sup id="fnref:16">&lt;a href="#fn:16" class="footnote-ref" role="doc-noteref">16&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>PyWPS &lt;sup id="fnref:17">&lt;a href="#fn:17" class="footnote-ref" role="doc-noteref">17&lt;/a>&lt;/sup> is used to provide the Least Cost Path
algorithm as a wps in combination with flask &lt;sup id="fnref:18">&lt;a href="#fn:18" class="footnote-ref" role="doc-noteref">18&lt;/a>&lt;/sup>. As client,
Birdy &lt;sup id="fnref:19">&lt;a href="#fn:19" class="footnote-ref" role="doc-noteref">19&lt;/a>&lt;/sup> connects to the wps, sends the cost raster, starting point,
end points and receives the resulting Least Cost Path. The initial
implementation of the Least Cost Path algorithm is based on the
implementation for the QGIS-Plugin &amp;lsquo;Least Cost
Path&amp;rsquo; &lt;sup id="fnref:20">&lt;a href="#fn:20" class="footnote-ref" role="doc-noteref">20&lt;/a>&lt;/sup> in version 1.0,
but refactored to optionally export the aggregated costs in a command
line tool. On top, the wps provides the complete Least Cost Path
algorithm as a single capability.&lt;/p>
&lt;p>In order to compute intermediate cost raster the different vector layers
of the different entities are optionally filtered, buffered and then
rasterised. Filtering the layers of the vector files for special
attributes enables further differentiation. For example, it is possible
to differentiate between heath and uncultivated land in the land cover.
Adding a buffer can be used either to convert a line object such as a
power line and a road into a polygon (with the correct physical width),
or to add minimum distance from an existing of planed area to the new
power line. Each of theses intermediate rasters are given a weight
(cost) which expresses the cost of using land covered by this layer. In
the final cost raster costs of all intermediate rasters are aggregated
with the maximum function. Thus, an area covered by several layers is
uniformly associated with the highest cost. Any place in the study area,
that is not covered by any layer and thus does not yet have a weight, is
given the default cost.&lt;/p>
&lt;p>The costs have been grouped into five different levels (see
table &lt;a href="#tab:1">1&lt;/a>, starting
from &lt;em>Preferential&lt;/em> areas with very low costs, via &lt;em>No restriction&lt;/em>s,
which is the default, used when no other layers are covering the local
area, to &lt;em>Restricted&lt;/em>, &lt;em>Strongly Restricted&lt;/em> and &lt;em>Prohibited&lt;/em> areas with
high costs. These higher costs represent the degree to which a place
with this cost should be avoided, while routing the path. The ratio of
the higher costs to the lower costs equals the detour the algorithm is
willing to go. Thus, as &lt;em>prohibited&lt;/em> areas describe a legal obligation,
not to use these areas or only to the utmost minimum, the weight that
resembles the costs for these types of areas is set especially high.&lt;br>
All these layers are provided as vectors. The Least Cost Path algorithm
uses raster data. Rasterisation transforms a vector into a raster.
Rasterisation can be done in two different ways. In both ways, the
rasterisation can be imaged, as the old vector is superimposed on the
new raster grid with the new given resolution and the new affine
transformation and the coordinate reference system of the vector. Both
rasterisation techniques differ in the selection of the pixel, that
describe the original polygon. A pixels will be selected, if either the
centre of that pixel is overlapped by the geo-object, or any part of the
pixel is overlaid. Setting all touched to True implies the version with
any part of the pixel selected. The version, where an overlapped pixel
centre is required, is setting the parameter all touched to False.
All touched set to False is considered the default.&lt;/p>
&lt;figure id="fig-least-cost-paths" style="text-align: center;">
&lt;div style="display: flex; justify-content: center; gap: 20px; flex-wrap: nowrap;">
&lt;div style="flex: 1; max-width: 45%;">
&lt;img src="https://sehheiden.github.io/paths/LeastCostPaths_al_F_v2_small.png" alt="All touched False" style="width: 100%; border: 1px solid #ddd; border-radius: 5px;">
&lt;figcaption style="text-align: center; margin-top: 5px;">All touched False.&lt;/figcaption>
&lt;/div>
&lt;div style="flex: 1; max-width: 45%;">
&lt;img src="https://sehheiden.github.io/paths/LeastCostPaths_al_T_v2_small.png" alt="All touched True" style="width: 100%; border: 1px solid #ddd; border-radius: 5px;">
&lt;figcaption style="text-align: center; margin-top: 5px;">All touched True.&lt;/figcaption>
&lt;/div>
&lt;/div>
&lt;figcaption style="text-align: center; font-style: italic; margin-top: 10px;">
&lt;strong>Abbildung 1:&lt;/strong> Figures of the Least Cost Paths contrasting the changes depending on the parameter *all touched*.
&lt;/figcaption>
&lt;/figure>
&lt;p>::: {#tab:1}&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Cost Level&lt;/th>
&lt;th>Cost&lt;/th>
&lt;th>Example&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Prohibited&lt;/td>
&lt;td>500&lt;/td>
&lt;td>National Parks, Buildings&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Strongly Restricted&lt;/td>
&lt;td>10&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Restricted&lt;/td>
&lt;td>5&lt;/td>
&lt;td>Industrial Areas, motorway, railway&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>No Restriction&lt;/td>
&lt;td>0.5&lt;/td>
&lt;td>Default&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Preferential&lt;/td>
&lt;td>0.1&lt;/td>
&lt;td>Motorway and Railway Buffers&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>: Used levels of costs, the applied numerical equivalent and example layer this cost have been used for.&lt;/p>
&lt;p>:::&lt;/p>
&lt;p>All three steps of the generation of the Least Cost Path: generation of
the cost raster, aggregation and backtracking is shown with an example
for a cost raster of 50 m resolution and all touched set to False (see
&lt;a href="#fig-costs2path">Figure 6&lt;/a>.&lt;/p>
&lt;p>The chosen implementation applies early stopping. Therefore, the costs
for points that are not needed to try to connect to the end point are
not aggregated (see figure
&lt;a href="#fig:aggregation">[fig:aggregation]&lt;/a>. After finding an aggregated cost for
every end point, the aggregation stops and the backtracking starts.
Because the path ends at a power transformer, which is a building type,
the paths end at in a &lt;em>Prohibited&lt;/em> area. Therefore, areas even further
away from the starting point have been explored first.&lt;/p>
&lt;figure id="fig-costs2path" style="text-align: center;">
&lt;div style="display: flex; justify-content: center; gap: 20px; flex-wrap: nowrap;">
&lt;div style="flex: 1; max-width: 30%;">
&lt;img src="https://sehheiden.github.io/paths/CostRasterExample_cut.png" alt="Cost Raster" style="width: 100%; border: 1px solid #ddd; border-radius: 5px;">
&lt;figcaption style="text-align: center; margin-top: 5px;">Cost Raster.&lt;/figcaption>
&lt;/div>
&lt;div style="flex: 1; max-width: 30%;">
&lt;img src="https://sehheiden.github.io/paths/AggregatedCosts_cut.png" alt="Aggregated Cost Raster" style="width: 100%; border: 1px solid #ddd; border-radius: 5px;">
&lt;figcaption style="text-align: center; margin-top: 5px;">Aggregated Cost Raster.&lt;/figcaption>
&lt;/div>
&lt;div style="flex: 1; max-width: 30%;">
&lt;img src="https://sehheiden.github.io/paths/LeastCostPathExample_cut.png" alt="Least Cost Path" style="width: 100%; border: 1px solid #ddd; border-radius: 5px;">
&lt;figcaption style="text-align: center; margin-top: 5px;">Least Cost Path.&lt;/figcaption>
&lt;/div>
&lt;/div>
&lt;figcaption style="text-align: center; font-style: italic; margin-top: 10px;">
&lt;strong>Figure 6:&lt;/strong> Figures of the cost raster and the resulting aggregated costs and the Least Cost Path for a resolution of 50 m, *all touched* set to False.
&lt;/figcaption>
&lt;/figure>
&lt;p>For low resolution rasterisation, with all touched set to True will show
every detail, but the objects are enlarged. When all touched is to False
the object only appears, when the is situated at the pixel centre. Thus,
this might be used as surrogate, that expresses the likelihood of the
object to be sampled and correlates with the object size compared to the
pixel size. At high resolution the set all touched to True still
overestimates the object size, but the extent is limited. Setting
all touched to False will include all objects for high resolution. This
setting is most realistic, because the over- and underestimation of the
object size is limited to half a pixel size in every direction. The best
method should be to use the percentage of the pixel coverage by the
object as weight, which is not possible. As an alternative, switching
between setting all touched True and False may result in a better
assessment of the true costs. When superimposing the resulting cost
raster, these map will include both aspects of the correctness: showing
every detail and statistically distribute better the real cost. Another
method to achieve the same is to downsample high resolution raster.&lt;/p>
&lt;h1 id="sec:results">Results&lt;/h1>
&lt;p>In this chapter we want to show the different cost rasters, that were
created from the same set of layers at different resolutions. The Least
Cost Path is estimated from this set of rasters. In the last step the
Least Cost Path is computed from the medium resolution rasters and
compared with the Least Cost Paths computed from a high resolution
raster.&lt;/p>
&lt;h2 id="subsec:cost-raster">Cost Raster&lt;/h2>
&lt;p>The cost raster contains all the costs for the geographical region of
the study area. The different intermediate cost rasters in the study
area are aggregated by the maximum function. If the resolution is higher
than the object size, then the effect of setting all touched to True or
False is limited. If all touched is set to True and any part of the
pixel that is covered by the object, the whole pixel is attributed to
the object. This makes the object appear larger. This can be seen in
figure &lt;a href="#fig-costs-5m">Figure 2&lt;/a>, which shows a detailed view of the costs for
the village of Beverstedt. To set all touched to False is a better
description of the real size of the object for high resolution.&lt;/p>
&lt;figure id="fig-costs-5m" style="text-align: center;">
&lt;div style="display: flex; justify-content: center; gap: 20px; flex-wrap: nowrap;">
&lt;div style="flex: 1; max-width: 45%;">
&lt;img src="https://sehheiden.github.io/paths/CostRaster_5m_alT_v2_small.png" alt="All touched: True" style="width: 100%; border: 1px solid #ddd; border-radius: 5px;">
&lt;figcaption style="text-align: center; margin-top: 5px;">All touched: True.&lt;/figcaption>
&lt;/div>
&lt;div style="flex: 1; max-width: 45%;">
&lt;img src="https://sehheiden.github.io/paths/CostRaster_5m_alF_v2_small.png" alt="All touched: False" style="width: 100%; border: 1px solid #ddd; border-radius: 5px;">
&lt;figcaption style="text-align: center; margin-top: 5px;">All touched: False.&lt;/figcaption>
&lt;/div>
&lt;/div>
&lt;figcaption style="text-align: center; font-style: italic; margin-top: 10px;">
&lt;strong>Abbildung 2:&lt;/strong> Part of the cost raster. Contrasting the different settings of *all touched* at a resolution of 5 m.
&lt;/figcaption>
&lt;/figure>
&lt;p>In contrast, if the resolution is smaller, all touched set to False
leads to a loss of information for smaller objects. Since the default
cost is much smaller than the average cost, this method underestimates
the cost. The figure &lt;a href="#fig:costs_100m">Figure 3&lt;/a> shows, that for the resolution of 100 m,
larger objects are still included in the map, but smaller objects, such
as roads, are only partially included.&lt;/p>
&lt;figure id="fig-costs-100m" style="text-align: center;">
&lt;div style="display: flex; justify-content: center; gap: 20px; flex-wrap: nowrap;">
&lt;div style="flex: 1; max-width: 45%;">
&lt;img src="https://sehheiden.github.io/paths/CostRaster_100m_alT_v2_small.png" alt="All touched: True" style="width: 100%; border: 1px solid #ddd; border-radius: 5px;">
&lt;figcaption style="text-align: center; margin-top: 5px;">All touched: True.&lt;/figcaption>
&lt;/div>
&lt;div style="flex: 1; max-width: 45%;">
&lt;img src="https://sehheiden.github.io/paths/CostRaster_100m_alF_v2_small.png" alt="All touched: False" style="width: 100%; border: 1px solid #ddd; border-radius: 5px;">
&lt;figcaption style="text-align: center; margin-top: 5px;">All touched: False.&lt;/figcaption>
&lt;/div>
&lt;/div>
&lt;figcaption style="text-align: center; font-style: italic; margin-top: 10px;">
&lt;strong>Abbildung 3:&lt;/strong> Part of the cost raster. Contrasting the different settings of *all touched* at a resolution of 100 m.
&lt;/figcaption>
&lt;/figure>
&lt;h2 id="subsec:least-cost-paths">Least Cost Paths&lt;/h2>
&lt;p>For each resolution the Least Cost Paths were estimated with the
all touched set to False and True.&lt;/p>
&lt;p>For the study, a starting point was chosen at a transformer about 6 km
north of the container terminal Bremerhaven and an end point at a
transformer in the southeast of the Osterholz county.&lt;/p>
&lt;p>The distance between the paths is calculated by the mean minimum
distance. For each vertex $P_i$ in the path $L_1$ the minimum distance
between the vertex $P_i$ and the path $L_2$ is calculated and then the
minimum distances are averaged (see
equation &lt;a href="#eq:1">[eq:1]&lt;/a>.
$$d_{mean} = \frac{1}{|L_1|} \sum_{i=1}^{n} d_{min}(P_i, L_2) \Bigr\vert P_i \in L_1$$
This equation is used to measure the degree of similarity between the
paths. The distances are measured at the same resolution (different
setting for all touched), and same setting of all touched (different
resolution).&lt;/p>
&lt;p>Table &lt;a href="#tab:2">[tab:2]&lt;/a> shows,
that the distance between two paths decreases with increasing
resolution. In addition, this tendency is depicted in
figure &lt;a href="#fig:paths_resolution">Figure 4&lt;/a> for the calculated cost paths of 5 m
and 100 m resolutions.&lt;/p>
&lt;p>At the same time, the differences in the aggregated costs remain almost
constant. Thus the difference between the aggregated costs per
resolution decreases. Setting all touched to False underestimates and to
True overestimates the costs.&lt;/p>
&lt;figure id="fig-paths-resolution" style="text-align: center;">
&lt;div style="display: flex; justify-content: center; gap: 20px; flex-wrap: nowrap;">
&lt;div style="flex: 1; max-width: 45%;">
&lt;img src="https://sehheiden.github.io/paths/LeastCostPaths_5m_v2_small.png" alt="Resolution of 5 m" style="width: 100%; border: 1px solid #ddd; border-radius: 5px;">
&lt;figcaption style="text-align: center; margin-top: 5px;">Resolution of 5 m.&lt;/figcaption>
&lt;/div>
&lt;div style="flex: 1; max-width: 45%;">
&lt;img src="https://sehheiden.github.io/paths/LeastCostPaths_100m_v2_small.png" alt="Resolution of 100 m" style="width: 100%; border: 1px solid #ddd; border-radius: 5px;">
&lt;figcaption style="text-align: center; margin-top: 5px;">Resolution of 100 m.&lt;/figcaption>
&lt;/div>
&lt;/div>
&lt;figcaption style="text-align: center; font-style: italic; margin-top: 10px;">
&lt;strong>Abbildung 4:&lt;/strong> Figures of the Least Cost Paths contrasting the paths for different resolutions. Paths with *all touched* set to False are indicated by dashed lines and True by continuous lines. Higher resolutions are indicated by green, lower resolutions by red. Using OpenStreetMaps as base map.
&lt;/figcaption>
&lt;/figure>
&lt;figure id="fig-paths-alltouched" style="text-align: center;">
&lt;div style="display: flex; justify-content: center; gap: 20px; flex-wrap: nowrap;">
&lt;div style="flex: 1; max-width: 45%;">
&lt;img src="https://sehheiden.github.io/paths/LeastCostPaths_al_F_v2_small.png" alt="All touched: False" style="width: 100%; border: 1px solid #ddd; border-radius: 5px;">
&lt;figcaption style="text-align: center; margin-top: 5px;">All touched: False.&lt;/figcaption>
&lt;/div>
&lt;div style="flex: 1; max-width: 45%;">
&lt;img src="https://sehheiden.github.io/paths/LeastCostPaths_al_T_v2_small.png" alt="All touched: True" style="width: 100%; border: 1px solid #ddd; border-radius: 5px;">
&lt;figcaption style="text-align: center; margin-top: 5px;">All touched: True.&lt;/figcaption>
&lt;/div>
&lt;/div>
&lt;figcaption style="text-align: center; font-style: italic; margin-top: 10px;">
&lt;strong>Abbildung 5:&lt;/strong> Figures of the Least Cost Paths contrasting the changes depending on the parameter *all touched*. Paths with *all touched* set to False are indicated by dashed lines and True by continuous lines. Higher resolutions are shown in green, lower resolutions in red. Using OpenStreetMaps as base map.
&lt;/figcaption>
&lt;/figure>
&lt;p>::: {#tab:2}&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>res /m&lt;/th>
&lt;th>$l_{al=f} /m$&lt;/th>
&lt;th>$l_{al=t} /m$&lt;/th>
&lt;th>$d_{mean}$ /m&lt;/th>
&lt;th>$d_{max}$ /m&lt;/th>
&lt;th>agg. $cost_{al=f}$&lt;/th>
&lt;th>agg. $cost_{al=t}$&lt;/th>
&lt;th>$\Delta$ costs&lt;/th>
&lt;th>agg. $costs_{al=f} \times m$&lt;/th>
&lt;th>agg. $costs_{al=t} \times m$&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>5&lt;/td>
&lt;td>76136.3&lt;/td>
&lt;td>78002.0&lt;/td>
&lt;td>126.0&lt;/td>
&lt;td>1065.0&lt;/td>
&lt;td>18665.9&lt;/td>
&lt;td>19616.8&lt;/td>
&lt;td>-850.00&lt;/td>
&lt;td>93329.6&lt;/td>
&lt;td>97584.8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>10&lt;/td>
&lt;td>75430.1&lt;/td>
&lt;td>77936.6&lt;/td>
&lt;td>277.9&lt;/td>
&lt;td>1590.0&lt;/td>
&lt;td>8931.2&lt;/td>
&lt;td>9731.2&lt;/td>
&lt;td>-799.95&lt;/td>
&lt;td>89312.5&lt;/td>
&lt;td>97311.8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>25&lt;/td>
&lt;td>75422.9&lt;/td>
&lt;td>78422.9&lt;/td>
&lt;td>313.8&lt;/td>
&lt;td>1621.2&lt;/td>
&lt;td>3354.9&lt;/td>
&lt;td>3872.7&lt;/td>
&lt;td>-517.78&lt;/td>
&lt;td>83871.7&lt;/td>
&lt;td>96816.4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>50&lt;/td>
&lt;td>76135.0&lt;/td>
&lt;td>70620.0&lt;/td>
&lt;td>1140.0&lt;/td>
&lt;td>4950.0&lt;/td>
&lt;td>1409.0&lt;/td>
&lt;td>2300.1&lt;/td>
&lt;td>-891.05&lt;/td>
&lt;td>70451.2&lt;/td>
&lt;td>115003.7&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>100&lt;/td>
&lt;td>76283.8&lt;/td>
&lt;td>74120.7&lt;/td>
&lt;td>1946.4&lt;/td>
&lt;td>6016.6&lt;/td>
&lt;td>640.5&lt;/td>
&lt;td>1572.3&lt;/td>
&lt;td>-931.70&lt;/td>
&lt;td>64051.6&lt;/td>
&lt;td>167226.8&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>:::&lt;/p>
&lt;p>When estimating the distance between the Least Costs Paths from
all touched set to True and all touched set to False at the same
resolution, the mean minimum distance between the 100 m resolution paths
is 1946.41 m and between the 5 m resolution paths is 126.04 m. The
similarity between the all touched set to False paths is higher, than
for setting to True. The distance for the 100 m path to the 5 m
resolution path is 243.42 m for all touched False and 2109.44 m for
all touched True.&lt;/p>
&lt;p>When cross comparing the similarity between all paths set with
all touched set to False and the similarity in between the same
resolution, the similarity in between the all touched set to False paths
is higher, than for most paths of the same resolution. Namely except for
the highest resolution.&lt;/p>
&lt;p>This behaviour is shown in
figure &lt;a href="#fig:paths_alltouched">6&lt;/a>. On a more detailed level, it can be
seen, that the paths of all touched False also converge directly to the
all touched True paths, but the extent is smaller.&lt;/p>
&lt;p>The zonal stat (see table &lt;a href="#tab:3">[tab:3]&lt;/a>) for a buffer of 100 m (5 m) around the paths has
been used, to estimate the percentage of each costs levels around the
paths. When using all touched True at higher resolution the tendency is
to use a higher percentage of the &lt;em>Preferential&lt;/em> Level and less of the
&lt;em>NoRestriction&lt;/em> Level. There is no strong tendency for the all touched
set to False Least Cost Paths.&lt;/p>
&lt;p>::: {#tab:3}&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>res /m&lt;/th>
&lt;th>all touched&lt;/th>
&lt;th>$r_{Preferential} %$&lt;/th>
&lt;th>$r_{No Restriction} %$&lt;/th>
&lt;th>$r_{Restricted} %$&lt;/th>
&lt;th>$r_{strongly Restricted} %$&lt;/th>
&lt;th>$r_{Prohibited} %$&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>5&lt;/td>
&lt;td>False&lt;/td>
&lt;td>4.7 (5.4)&lt;/td>
&lt;td>58.7 (58.9)&lt;/td>
&lt;td>8.8 (8.4)&lt;/td>
&lt;td>0.7 (0.7)&lt;/td>
&lt;td>27.1 (26.7)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>10&lt;/td>
&lt;td>False&lt;/td>
&lt;td>19.6 (33.5)&lt;/td>
&lt;td>68.5 (64.5)&lt;/td>
&lt;td>1.0 (0.8)&lt;/td>
&lt;td>0.8 (0.3)&lt;/td>
&lt;td>10.1 (0.9)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>25&lt;/td>
&lt;td>False&lt;/td>
&lt;td>19.2 (34.2)&lt;/td>
&lt;td>68.9 (64.9)&lt;/td>
&lt;td>1.0 (0.2)&lt;/td>
&lt;td>0.7 (0.1)&lt;/td>
&lt;td>9.7 (0.6)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>50&lt;/td>
&lt;td>False&lt;/td>
&lt;td>20.4 (33.2)&lt;/td>
&lt;td>68.0 (66.2)&lt;/td>
&lt;td>0.9 (0.1)&lt;/td>
&lt;td>0.7 (0.0)&lt;/td>
&lt;td>10.1 (0.5)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>100&lt;/td>
&lt;td>False&lt;/td>
&lt;td>21.1 (30.7)&lt;/td>
&lt;td>69.1 (68.8)&lt;/td>
&lt;td>1.1 (0.0)&lt;/td>
&lt;td>0.7 (0.0)&lt;/td>
&lt;td>7.9 (0.4)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>5&lt;/td>
&lt;td>True&lt;/td>
&lt;td>18.9 (28.5)&lt;/td>
&lt;td>67.3 (66.4)&lt;/td>
&lt;td>1.3 (1.6)&lt;/td>
&lt;td>1.0 (0.5)&lt;/td>
&lt;td>11.5 (3.0)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>10&lt;/td>
&lt;td>True&lt;/td>
&lt;td>18.9 (33.7)&lt;/td>
&lt;td>66.6 (63.4)&lt;/td>
&lt;td>1.6 (1.4)&lt;/td>
&lt;td>1.4 (0.6)&lt;/td>
&lt;td>11.5 (1.0)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>25&lt;/td>
&lt;td>True&lt;/td>
&lt;td>18.7 (31.9)&lt;/td>
&lt;td>65.5 (65.5)&lt;/td>
&lt;td>2.0 (1.3)&lt;/td>
&lt;td>2.5 (0.7)&lt;/td>
&lt;td>11.4 (0.6)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>50&lt;/td>
&lt;td>True&lt;/td>
&lt;td>9.1 (13.0)&lt;/td>
&lt;td>75.7 (83.0)&lt;/td>
&lt;td>3.9 (2.0)&lt;/td>
&lt;td>4.2 (1.6)&lt;/td>
&lt;td>7.1 (0.4)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>100&lt;/td>
&lt;td>True&lt;/td>
&lt;td>7.0 (10.1)&lt;/td>
&lt;td>73.8 (81.9)&lt;/td>
&lt;td>5.5 (3.9)&lt;/td>
&lt;td>8.5 (3.6)&lt;/td>
&lt;td>5.2 (0.4)&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>:::&lt;/p>
&lt;h2 id="subsec:execution-time">Execution time&lt;/h2>
&lt;p>In theory, the execution time increases by power of two with the
resolution, because higher resolutions result in a higher number of
pixels. A double logarithmic fit shows, that the execution time scales
with power of $2.1997 \pm 0.007$ of the inverse resolution.&lt;/p>
&lt;p>The total execution time consists of two parts: the aggregation of the
costs and the back tracking of the least cost to find the path.&lt;/p>
&lt;h2 id="subsec:faster-processing-of-the-cost-path-algorithm">Faster Processing of the Cost Path Algorithm&lt;/h2>
&lt;p>The first step is to optimise the computational speed by a reduced area.
Another method is to improve the prediction of the medium resolution
itself and thus reduce the need for a computation in higher resolution.&lt;/p>
&lt;h3 id="compare-least-cost-paths-from-rasters-of-both-alltouched-settings">Compare Least Cost Paths from rasters of both all touched settings&lt;/h3>
&lt;p>For the example paths shown, all touched set to True overestimates ans
set to False underestimates the True costs.&lt;/p>
&lt;p>A weighted average of the costs could therefore be a more accurate
measure and make estimated medium resolution Least Cost Path more
similar to high resolution paths. As above example shows the weighting
should favour all touched to False.&lt;/p>
&lt;p>This will speed-up the aggregation. The time needed for the back
tracking stays unchanged.&lt;/p>
&lt;p>The optimal ratio of overlaying both all touched for cost raster with
10 m resolution is estimated via similarity of the resulting Least Cost
Path to the path of the original high resolution raster. The mean
distance of Least Cost Paths from rasters with different ratio is
estimated to the path from the all touched set to False raster of the
higher (5 m) resolution. Table &lt;a href="#tab:4">2&lt;/a> shows, that the mean minimum distance decreases with
increasing ratio (1:1, 2:1, 4:1) and after this optimum is reached,
increases with increasing ratio (8:1, 16:1 and so on). Comparing the
similarity of the paths of the different ratios to normal paths with
10 m resolution shows, that paths with a higher ratio of all touched set
to True are nearer to the all touched set to True paths. Paths with a
ratio in favour of all touched False are much closer to the all touched
set to False paths.&lt;/p>
&lt;p>::: {#tab:4}&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>r&lt;/th>
&lt;th>$d_{5~al=f}$ /m&lt;/th>
&lt;th>$d_{10~al=f}$ /m&lt;/th>
&lt;th>$d_{10~al=t}$ /m&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>1:1&lt;/td>
&lt;td>119.6&lt;/td>
&lt;td>285.5&lt;/td>
&lt;td>47.2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2:1&lt;/td>
&lt;td>97.1&lt;/td>
&lt;td>263.5&lt;/td>
&lt;td>74.2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>4:1&lt;/td>
&lt;td>40.1&lt;/td>
&lt;td>206.4&lt;/td>
&lt;td>100.2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>8:1&lt;/td>
&lt;td>41.7&lt;/td>
&lt;td>169.0&lt;/td>
&lt;td>137.3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>16:1&lt;/td>
&lt;td>56.7&lt;/td>
&lt;td>153.3&lt;/td>
&lt;td>152.7&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>32:1&lt;/td>
&lt;td>56.7&lt;/td>
&lt;td>145.6&lt;/td>
&lt;td>162.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>64:1&lt;/td>
&lt;td>163.5&lt;/td>
&lt;td>10.6&lt;/td>
&lt;td>272.4&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>: Paths computed from the overlaying of all touched set to False and True raster with the mean minimum distance (d) of the paths to the paths calculated from the all touched set to False 5 m resolution and all touched set to False and True raster at 10 m resolution with the ratio (r).&lt;/p>
&lt;p>:::&lt;/p>
&lt;h3 id="compare-least-cost-paths-from-downsampled-cost-raster">Compare Least Cost Paths from downsampled cost raster&lt;/h3>
&lt;p>As an alternative to the superposition of the rasters of same
resolution, a high resolution (5 m) all touched False raster is
downsampled to 10 m, 25 m, 50 m and 100 m (bi-linear) interpolation.&lt;/p>
&lt;p>The distances from the paths that are computed from the bi-linear
downsampled raster to the path of the original 5 m resolution (all
touched False) shows, that only downsampling to a resolution of 10 m
produces a path that is relatively close the high resolution paths (see
table &lt;a href="#tab:5">3&lt;/a>).&lt;/p>
&lt;p>The opposite is true for the lower resolution raster which is more
similar to paths computed from the all touched True cost raster. Every
path from a downsampled raster is more similar to a path computed from
an all touched set to True raster, than to an all touched set to False
raster, although the all touched set to False raster of the 5 m
resolution was used for downsampling.&lt;/p>
&lt;p>::: {#tab:5}&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>res /m&lt;/th>
&lt;th>l /m&lt;/th>
&lt;th>$d_{5~m}$ /m&lt;/th>
&lt;th>$d_{al=f}$ /m&lt;/th>
&lt;th>$d_{al=t}$ /m&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>10&lt;/td>
&lt;td>75980.6&lt;/td>
&lt;td>59.3&lt;/td>
&lt;td>219.4&lt;/td>
&lt;td>143.6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>25&lt;/td>
&lt;td>70205.3&lt;/td>
&lt;td>385.8&lt;/td>
&lt;td>558.1&lt;/td>
&lt;td>432.8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>50&lt;/td>
&lt;td>69217.9&lt;/td>
&lt;td>730.8&lt;/td>
&lt;td>693.4&lt;/td>
&lt;td>255.7&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>100&lt;/td>
&lt;td>66667.9&lt;/td>
&lt;td>1681.3&lt;/td>
&lt;td>1605.6&lt;/td>
&lt;td>400.6&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>: Length (l) of the path computed from the bi-linear downsampled raster and the mean minimum distance (d) of the paths from the downsampled raster to the paths calculated from the all touched set to True and False raster of the same resolution as the downsampled raster with resolution (res).&lt;/p>
&lt;p>:::&lt;/p>
&lt;h3 id="restrict-search-to-a-buffer-around-the-least-cost-paths">Restrict search to a buffer around the Least Cost Paths&lt;/h3>
&lt;p>Construct a polygon from the two Least Cost Paths (all touched True and
all touched False of the same resolution). Buffer the polygon with twice
the maximum minimum path distance (see equation &lt;a href="#eq:2">[eq:2]&lt;/a>).
$$d_{max} = max(\sum_{i=1}^{n} d_{min}(P_i, L_2)) \Bigr\vert P_i \in L_1$$&lt;/p>
&lt;p>This strategy results in the same Least Cost Path as with the original
high resolution raster.&lt;/p>
&lt;p>This reduction of compute power provides the possibility to run a 2.5 m.
This clipped 2.5 m raster for all touched set to True changes the path
only slightly.&lt;/p>
&lt;p>The all touched False raster, on the other hand, leads to a completely
new previously unused subroute at the end of the path. Due to the low
resolution a small path became passable. This small path is a power line
next to a road between protected landscape areas. The road and the
protected landscape areas are both &lt;em>restricted&lt;/em> areas, while the power
line is &lt;em>preferred&lt;/em>.&lt;/p>
&lt;h3 id="apply-the-proposed-solutions-on-other-paths">Apply the proposed solutions on other paths&lt;/h3>
&lt;p>To broaden the view and verify the result, four different routes should
be found using the above strategies. Two routes should be found from the
starting point to two new points in the south east of the investigated
area and two routes should be found from the north and north east of the
study area to the end point.&lt;/p>
&lt;p>For three of the four routes, the Least Cost Path estimation from the
clipped raster was able to calculate exactly the same result. For the
fourth path, the Least Cost Path from the 5 m resolution raster was
clipped by the buffer around the 50 m resolution paths. The speed up
from the clipping of the higher resolution raster depends on the number
of pixels that have been clipped.&lt;/p>
&lt;p>Bi-linear downsampling the of the high resolution raster to a medium
resolution did not result in any benefits compared to an original medium
resolution raster. The aggregated cost per resolution of the Least Cost
Path from the downsampled raster is higher than the normal medium
resolution 10 m raster. In addition, the distance from these paths to
the high resolution path is greater than the distance from the original
10 m resolution path to the 5 m resolution path (see
table &lt;a href="#tab:6">4&lt;/a>).&lt;/p>
&lt;p>::: {#tab:6}&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Route&lt;/th>
&lt;th>Method&lt;/th>
&lt;th>$length /m$&lt;/th>
&lt;th>$costs_{al=f}$&lt;/th>
&lt;th>$d_{mean}$ /m&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>P1-E&lt;/td>
&lt;td>5 m&lt;/td>
&lt;td>107889.6&lt;/td>
&lt;td>208547.8&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>Clipped&lt;/td>
&lt;td>107889.6&lt;/td>
&lt;td>208547.8&lt;/td>
&lt;td>0.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>Down&lt;/td>
&lt;td>96754.2&lt;/td>
&lt;td>212911.0&lt;/td>
&lt;td>628.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>10 m&lt;/td>
&lt;td>107232.9&lt;/td>
&lt;td>203010.2&lt;/td>
&lt;td>103.5&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>P2-E&lt;/td>
&lt;td>5 m&lt;/td>
&lt;td>103706.4&lt;/td>
&lt;td>155567.9&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>Clipped&lt;/td>
&lt;td>103706.4&lt;/td>
&lt;td>155567.9&lt;/td>
&lt;td>0.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>Down&lt;/td>
&lt;td>92403.3&lt;/td>
&lt;td>158238.6&lt;/td>
&lt;td>639.9&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>10 m&lt;/td>
&lt;td>104249.9&lt;/td>
&lt;td>149899.7&lt;/td>
&lt;td>177.7&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>S-P3&lt;/td>
&lt;td>5 m&lt;/td>
&lt;td>102187.1&lt;/td>
&lt;td>34503.8&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>Clipped&lt;/td>
&lt;td>90377.1&lt;/td>
&lt;td>37926.1&lt;/td>
&lt;td>4465.4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>Down&lt;/td>
&lt;td>94125.6&lt;/td>
&lt;td>37574.9&lt;/td>
&lt;td>742.4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>10 m&lt;/td>
&lt;td>102461.6&lt;/td>
&lt;td>32446.0&lt;/td>
&lt;td>81.2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>S-P4&lt;/td>
&lt;td>5 m&lt;/td>
&lt;td>96449.2&lt;/td>
&lt;td>33865.5&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>Clipped&lt;/td>
&lt;td>96449.2&lt;/td>
&lt;td>33865.5&lt;/td>
&lt;td>0.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>Down&lt;/td>
&lt;td>87861.1&lt;/td>
&lt;td>36462.7&lt;/td>
&lt;td>796.4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>10 m&lt;/td>
&lt;td>96739.5&lt;/td>
&lt;td>31899.3&lt;/td>
&lt;td>83.5&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>: Length (l) of the path, the aggregated costs per resolution and the mean minimum distance ($d_{mean}$) to the path created from the 5 m resolution all touched False raster for the four control routes. For the reference path constructed from 5 m and 10 m raster and from 5 m to 10 m downsampled raster and 5 m clipped raster for the routes point1 to end point (P1-E), point2 to end point (P2-E), starting point to point3 (S-P3) and starting point to point4 (S-P4).&lt;/p>
&lt;p>:::&lt;/p>
&lt;h1 id="sec:discussion">Discussion&lt;/h1>
&lt;p>In theory, the need for computing time increases with the resolution as
power of two. Similarly, the use of main memory increases. This again
limits the number of data points, that can be processed and the
resolution, and probably causes difference in increase of computation
time from power of 2 to a power of 2.2, because additional slower ram
moduls has been used for the aggregation of higher resolution.&lt;/p>
&lt;p>On the other hand, the similarity to higher resolutions only scales
linear with the resolution. Thus, there is a diminishing return of
smaller errors, compared to compute time and resources used.&lt;/p>
&lt;p>Therefore, this work attempts a) to reduce the computation power needed
and b) reduce the deviation for a given resolution compared to a higher
resolution raster.&lt;/p>
&lt;p>For a), to reduce the computational complexity, clipping of the high
resolution has been applied, to reduce the search space of the
aggregation. While this method reduces the computation time and memory
usage, the backtracking part stays unchanged.&lt;/p>
&lt;p>For b), two methods have been used to increase the similarity of the
paths computed from the medium resolution raster, to the paths of the
highest resolution raster. These methods are used as surrogates for the
more complex calculation of the Least Cost Path with the higher
resolution. In the first method a bi-linear downsampling of the higher
resolution raster has been applied and in the second method, the
all touched set to False and True rasters were averaged in different
ratios, to compute the optimum weighted cost raster. While the second
method of using an averaged raster, shows a higher similarity to the
path from the highest resolution raster, the downsampling method is
simpler and does not need to be optimised for the given cost. This
disadvantage could be reduced by normalising the costs. The fact that
downsampling leads to paths that are closer to all touched True, can be
attributed to the fact, that all objects are included, such as in to
all touched True.&lt;/p>
&lt;p>This shows that the way the cost raster is created in the first place
can play a crucial role in the end result. So, a nuance can cause a
detour. When this behaviour occurs, the polygon may not include the
Least Cost Path. This polygon should therefore be overlapped with a
polygon around the shortest path. For the set of control paths
downsampling could not outperform the original medium resolution
rasters.&lt;/p>
&lt;p>Early stopping may result in suboptimal paths around the end points for
some edge cases, where the connection via another neighbour might be
more optimal.&lt;/p>
&lt;p>The set of rules, that are used to create the intermediate cost raster,
includes a rule to create buffer around buildings which is set to the
level &lt;em>Prohibited&lt;/em> areas. In all touched set to False rasters the
resolution of the medium level raster needs to be high enough to show
every detail, at least in the magnitude of the minimum object size plus
twice its buffer. This is true for the 10 m resolution raster and less
true for the 25 m resolution raster, that misses some details for roads
for all touched False raster. Other details such as rivers and houses
are already included in the lower resolution raster, due to larger
buffers. The raster with all touched set to False might miss some
objects, but the missing chance is propotional to the object size and
the extent of overestimation is reduced. The Least Cost Path algorithm
searches for an optimal path as a line. As lines do not have a width,
the route found might contain bottlenecks, that have a smaller width
than the object that should be placed there. Therefore, the used
resolution should not be smaller than the width of the object that
should be placed. This can not be avoided by downsampling, but by
weighting the medium resolution all touched True and False rasters.&lt;/p>
&lt;p>This paper examines the effect of computational costs and deviation of
the results for a limited set of points. Also, only the cost of finding
the Least Cost Path from a single starting point to a single end point
has been considered. If multiple endpoints are used, the computational
cost for the aggregated cost raster has to be paid only once.&lt;/p>
&lt;p>If multiple paths are calculated from a single raster, the speed-up
benefit is reduced. Especially pre-calculation on medium resolution
raster and clipping around a buffering of the resulting medium
resolution paths becomes less effective, because as the number of paths
increases, fewer pixels are clipped. The Least Cost Path algorithm does
only select the single most cost-effective path. Therefore, paths of
similar, but slightly higher costs remain unknown. In additional, slight
variations on the costs rasters can lead to very different paths,
although the costs will not change much. End-users may be interested in
selecting a path from a set of similar aggregated costs and applying
their own evaluation criteria. This can be achieved a by adjusting the
backtracking and return polygons, or by applying perturbation on the
costs.&lt;/p>
&lt;p>In this work, the intermediate cost layers are aggregated by the maximum
function. Other possible aggregation functions are the sum and average
functions. Each aggregation function can be justified by a different
interpretation of the cost and its scale.&lt;/p>
&lt;p>When the &lt;em>Prohibited&lt;/em> level is used as the highest level, then summing
the two highest levels would result in a new highest level. Also, the
maximum function does not interfere with the nodata value. This can be
done by a nansum- / nanmean-function, if the nodata value is set to &lt;em>not
a number&lt;/em> during the aggregation. The disadvantage of aggregation with
the maximum function is, that this aggregation is unable to distinguish
between nuances of different overlapping intermediate costs. On sum or
average aggregated rasters, one can distinguish between different
sublevels.&lt;/p>
&lt;p>All touched False rasters produce more similar results than all touched
True raster, which is probably due to the fact, that the default level
is relatively low. As the default level increases, the effect would
probably be reduced for low resolution rasters. For high resolution
raster, the effect would still be present, because the use of the pixel
centre for sampling, reflects the original geometry better.&lt;/p>
&lt;p>This effect of the similar aggregated costs per resolution could also be
seen in the test paths, even when the paths varied greatly. This could
be an indication of an even spatial distribution of the costs.&lt;/p>
&lt;p>The all touched set to True cost raster shows every detail, but the
sampling with all touched set to True increases the size of the
features. The fact that the aggregated costs per resolution for all
touched True rasters overestimate the costs when computing the path from
a low resolution raster, might be due to the fact, that the high costs
are much more frequent, as they are exponential scaled.&lt;/p>
&lt;h1 id="sec:conclusion">Conclusion&lt;/h1>
&lt;p>The computing costs of the Least Cost Path scales with the square of the
resolution. The difference between the aggregated costs of the paths per
resolution computed from the all touched set to False raster and the
all touched set to True raster only shows a linear decrease. Therefore,
the gain in accuracy per compute time decreases. The presented methods
attempt to circumvent this bottleneck. When these strategies are applied
to a medium resolution raster, the compute time for high resolution
results can successfully be reduced without compromising the run time
for worse distances to the Least Cost Path from the high resolution
raster. The paths can change significantly even for a small change in
total cost. Therefore, developing an alternative backtracking algorithm,
that generates a corridor of costs might be a good strategy to offer
alternative paths and at the same time show variations between these
paths. An alternative would be to specify a range of costs and compute
the aggregation of the costs and than superimpose these aggregations and
compute the path by backtracking from the superimposed cost raster. This
again increases the computation time.&lt;/p>
&lt;p>In an actual search for the Least Cost Path of a power, a survey could
be used as a method to estimate the costs. If the sample size of the
survey is large enough, the weights could take into account local
differences in relative acceptance.&lt;/p>
&lt;p>Only changes in the algorithm have been applied in the search for
acceleration. Therefore, other methods such as just-in-time compilation
have not been tested.&lt;/p>
&lt;h1 id="references">References&lt;/h1>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>Eßer-Frey, Anke: Analyzing the regional long-term development of the German power system using a nodal pricing approach. &lt;a href="http://dx.doi.org/10.5445/IR/1000028367">http://dx.doi.org/10.5445/IR/1000028367&lt;/a>. Version: 2012&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>Leuthold, Florian U. ; Rumiantseva, Ina ; Weigt, Hannes ; JesKe, Till ;HiRschhausen, Christian von: Nodal Pricing in the German Electricity Sector -A Welfare Economics Analysis, with Particular Reference to Implementing Off-shore Wind Capacities. In: SSRN Electronic Journal (2005). &lt;a href="http://dx.doi.org/10.2139/ssrn.1137382">http://dx.doi.org/10.2139/ssrn.1137382&lt;/a>. – DOI 10.2139/ssrn.1137382. – ISSN 1556–5068&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>Suleiman, Sani ; AgaRwal, V C. ; Lal, Deepak ; Sunusi, Aminuddeen: Optimal Route Location by Least Cost Path (LCP) Analysis using (GIS), A Case Study. In: International Journal of Scientific Engineering and Technology Research 4 (2015), Oktober, Nr. 44, S. 9621–9626&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4">
&lt;p>Schäfer, Benjamin ; Pesch, Thiemo ; ManiK, Debsankha ; Gollenstede, Julian; Lin, Guosong ; BecK, Hans-Peter ; Witthaut, Dirk ; Timme, Marc: Understanding Braess’ Paradox in power grids. In: Nature Communications 13 (2022), September, Nr. 1, 5396. &lt;a href="http://dx.doi.org/10.1038/s41467-022-32917-6">http://dx.doi.org/10.1038/s41467-022-32917-6&lt;/a>. – DOI10.1038/s41467–022–32917–6. – ISSN 2041–1723. – Number: 1 Publisher: NaturePublishing Group&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5">
&lt;p>Nettostromerzeugung in Deutschland 2020: erneuerbare Energien erstmals über 50 Prozent - Fraunhofer ISE. &lt;a href="https://www.ise.fraunhofer.de/de/presse-und-medien/news/2020/nettostromerzeugung-in-deutschland-2021-erneuerbare-energien-erstmals-ueber-50-prozent.html">https://www.ise.fraunhofer.de/de/presse-und-medien/news/2020/nettostromerzeugung-in-deutschland-2021-erneuerbare-energien-erstmals-ueber-50-prozent.html&lt;/a>. Version: Januar 2021&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6">
&lt;p>Bertsch, Valentin ; Fichtner, Wolf: A participatory multi-criteria approach for power generation and transmission planning. In: Annals of Operations Research 245 (2016), Oktober, S. 177–207. &lt;a href="http://dx.doi.org/10.1007/s10479-015-1791-y">http://dx.doi.org/10.1007/s10479-015-1791-y&lt;/a>. – DOI 10.1007/s10479–015–1791–y&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&amp;#160;&lt;a href="#fnref1:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&amp;#160;&lt;a href="#fnref2:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&amp;#160;&lt;a href="#fnref3:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&amp;#160;&lt;a href="#fnref4:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7">
&lt;p>Dietrich, Kristin ; Leuthold, Florian ; Weigt, Hannes: Will the Market Get it Right? The Placing of New Power Plants in Germany. In: Zeitschrift für Energiewirtschaft 34 (2010), Dezember, Nr. 4, 255–265. &lt;a href="http://dx.doi.org/10.1007/s12398-010-0026-9">http://dx.doi.org/10.1007/s12398-010-0026-9&lt;/a>. – DOI 10.1007/s12398–010–0026–9. – ISSN 1866–2765&amp;#160;&lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8">
&lt;p>Hauff, Jochen ; Heider, Conrad ; ARms, Hanjo ; GeRbeR, Jochen ; Schilling, Martin: Gesellschaftliche Akzeptanz als Säule der energiepolitischen Zielsetzung. In: ET, Energiewirtschaftliche Tagesfragen 61 (2011), Oktober, Nr. 10, 85–87. &lt;a href="https://www.osti.gov/etdeweb/biblio/21522981">https://www.osti.gov/etdeweb/biblio/21522981&lt;/a>&amp;#160;&lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:9">
&lt;p>Butler, John ; Jia, Jianmin ; DyeR, James: Simulation techniques for the sensitivity analysis of multi-criteria decision models. In: European Journal of Operational Research 103 (1997), Dezember, Nr. 3, 531–546. &lt;a href="http://dx.doi.org/10.1016/S0377-2217(96)00307-4">http://dx.doi.org/10.1016/S0377-2217(96)00307-4&lt;/a>. – DOI 10.1016/S0377–2217(96)00307–4. – ISSN 0377–2217&amp;#160;&lt;a href="#fnref:9" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:10">
&lt;p>Bertsch, Valentin ; Treitz, Martin ; GeldeRmann, Jutta ; Rentz, Otto: Sensitivity analyses in multi-attribute decision support for off-site nuclear emergency and recovery management. In: International Journal of Energy Sector Management 1 (2007), Mai, Nr. 4, S. 342–365. &lt;a href="http://dx.doi.org/10.1108/17506220710836075">http://dx.doi.org/10.1108/17506220710836075&lt;/a>. – DOI 10.1108/17506220710836075&amp;#160;&lt;a href="#fnref:10" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:11">
&lt;p>Dijkstra, E. W.: A note on two problems in connexion with graphs. In: Numerische Mathematik 1 (1959), Dezember, Nr. 1, 269–271. &lt;a href="http://dx.doi.org/10.1007/BF01386390">http://dx.doi.org/10.1007/BF01386390&lt;/a>. – DOI 10.1007/BF01386390. – ISSN 0945–3245&amp;#160;&lt;a href="#fnref:11" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:12">
&lt;p>Schutzgebiete in Deutschland. &lt;a href="https://geodienste.bfn.de/schutzgebiete?lang=de">https://geodienste.bfn.de/schutzgebiete?lang=de&lt;/a>. Version: 2015&amp;#160;&lt;a href="#fnref:12" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:13">
&lt;p>Digitales Landschaftsmodell 1:250 000 (Ebenen). &lt;a href="https://gdz.bkg.bund.de/index.php/default/open-data/digitales-landschaftsmodell-1-250-000-ebenen-dlm250-ebenen.html">https://gdz.bkg.bund.de/index.php/default/open-data/digitales-landschaftsmodell-1-250-000-ebenen-dlm250-ebenen.html&lt;/a>. Version: 2021&amp;#160;&lt;a href="#fnref:13" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:14">
&lt;p>Boeing, Geoff: OSMnx: New methods for acquiring, constructing, analyzing, and visualizing complex street networks. In: Computers, Environment and Urban Systems 65 (2017), September, 126–139. &lt;a href="http://dx.doi.org/10.1016/j.compenvurbsys.2017.05.004">http://dx.doi.org/10.1016/j.compenvurbsys.2017.05.004&lt;/a>. – DOI 10.1016/j.compenvurbsys.2017.05.004. – ISSN 0198–9715&amp;#160;&lt;a href="#fnref:14" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:15">
&lt;p>OpenGeoData.NI. &lt;a href="https://opengeodata.lgln.niedersachsen.de/#lod1">https://opengeodata.lgln.niedersachsen.de/#lod1&lt;/a>. Version: 2022&amp;#160;&lt;a href="#fnref:15" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:16">
&lt;p>Metropolplaner. &lt;a href="https://metropolplaner.de/metropolplaner/">https://metropolplaner.de/metropolplaner/&lt;/a>. Version: 2022&amp;#160;&lt;a href="#fnref:16" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:17">
&lt;p>Welcome to the PyWPS 4.3.dev0 documentation! — PyWPS 4.3.dev0 documentation. &lt;a href="https://pywps.readthedocs.io/en/latest/index.html">https://pywps.readthedocs.io/en/latest/index.html&lt;/a>. Version: 2016&amp;#160;&lt;a href="#fnref:17" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:18">
&lt;p>Flask. &lt;a href="https://palletsprojects.com/p/flask/">https://palletsprojects.com/p/flask/&lt;/a>&amp;#160;&lt;a href="#fnref:18" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:19">
&lt;p>Birdy — Birdy 0.8.1 documentation. &lt;a href="https://birdy.readthedocs.io/en/latest/">https://birdy.readthedocs.io/en/latest/&lt;/a>&amp;#160;&lt;a href="#fnref:19" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:20">
&lt;p>LeastCostPath/dijkstra_algorithm.py at master · Gooong/LeastCostPath. &lt;a href="https://github.com/Gooong/LeastCostPath">https://github.com/Gooong/LeastCostPath&lt;/a>. Version: Dezember 2022&amp;#160;&lt;a href="#fnref:20" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div>
- https://sehheiden.github.io/posts/cableroute/ -</description></item></channel></rss>